{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             ____        __  _                     \n",
      "            / __ \\____  / /_(_)___ ___  __  _______\n",
      "           / / / / __ \\/ __/ / __ `__ \\/ / / / ___/\n",
      "          / /_/ / /_/ / /_/ / / / / / / /_/ (__  ) \n",
      "          \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  \n",
      "              /_/                                  \n",
      "              \n",
      "Just checking that all necessary environments vars are present...\n",
      "-----\n",
      "PYSPARK_PYTHON=python\n",
      "SPARK_HOME=C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\n",
      "JAVA_HOME=C:\\java8\n",
      "-----\n",
      "Starting or getting SparkSession and SparkContext...\n",
      "Setting checkpoint folder ( local ). If you are in a cluster initialize optimus with master='your_ip' as param\n",
      "Deleting previous folder if exists...\n",
      "Creating the checkpoint directory...\n",
      "Optimus successfully imported. Have fun :).\n"
     ]
    }
   ],
   "source": [
    "# Create optimus\n",
    "op = Optimus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "df = op.create.df(\n",
    "    [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True),\n",
    "                (\"animals\", \"str\", True),\n",
    "                (\"thing\", StringType(), True),\n",
    "                (\"two strings\", StringType(), True),\n",
    "                (\"filter\", StringType(), True),\n",
    "                (\"num 2\", \"string\", True),\n",
    "                (\"date\", \"string\", True),\n",
    "                (\"num 3\", \"str\", True)\n",
    "                \n",
    "            ],[\n",
    "                (\"  I like     fish  \", 1, \"dog\", \"&^%$#housé\", \"cat-car\", \"a\",\"1\", \"20150510\", '3'),\n",
    "                (\"    zombies\", 2, \"cat\", \"tv\", \"dog-tv\", \"b\",\"2\", \"20160510\", '3'),\n",
    "                (\"simpsons   cat lady\", 2, \"frog\", \"table\",\"eagle-tv-plus\",\"1\",\"3\", \"20170510\", '4'),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"c\",\"4\", \"20180510\", '5'),\n",
    "    \n",
    "            ]\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Optimus' object has no attribute 'DataFrameTransformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-9d4dc4100151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrameTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mactual_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"num1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"integer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Optimus' object has no attribute 'DataFrameTransformer'"
     ]
    }
   ],
   "source": [
    "source_df = op.create.df(\n",
    "    rows=[\n",
    "        (\"cafe\", 1),\n",
    "        (\"discoteca\", 2)\n",
    "    ],\n",
    "    cols=[\n",
    "        (\"place\", StringType(), True),\n",
    "        (\"num1\", IntegerType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "func = lambda num: num * 2\n",
    "actual_df = source_df.cols().apply(\"num1\", func, \"udf\")\n",
    "\n",
    "transformer = op.DataFrameTransformer(source_df)\n",
    "\n",
    "actual_df = transformer.set_col(\"num1\", func, \"integer\").df\n",
    "\n",
    "expected_df = op.create.df(\n",
    "    rows=[\n",
    "        (\"cafe\", 2),\n",
    "        (\"discoteca\", 4)\n",
    "    ],\n",
    "    cols=[\n",
    "        (\"place\", StringType(), True),\n",
    "        (\"num1\", IntegerType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert (expected_df.collect() == actual_df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  6|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  6|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  6|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  6|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().lookup('num',['3',4], 6).show()\n",
    "df.cols().lookup('num',[('3',6),(2,6)]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 'int', 'num 3': 'string'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cols().dtypes(['num','num 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('words', 'string'),\n",
       " ('num', 'int'),\n",
       " ('animals', 'string'),\n",
       " ('thing', 'string'),\n",
       " ('two strings', 'string'),\n",
       " ('filter', 'string'),\n",
       " ('num 2', 'string'),\n",
       " ('date', 'string'),\n",
       " ('num 3', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimus.helpers.functions import parse_columns\n",
    "parse_columns(df,'*', filter_by_type = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_df.cols().filter_by_type('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "source_df = op.create.df(\n",
    "    cols =[\n",
    "            (\"name\", StringType(), True),\n",
    "            (\"age\", IntegerType(), False)\n",
    "        ],\n",
    "        rows =[\n",
    "            (\"BOB\", 1),\n",
    "            (\"JoSe\", 2)\n",
    "        ]\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|new_num|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|      2|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|      3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|      3|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|      4|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "\n",
    "# Input/output are both a single double value\n",
    "@udf('int')\n",
    "def plus_one(v):\n",
    "    return v+1\n",
    "\n",
    "df.withColumn('new_num', plus_one(df.num)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|new_num|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|    2.0|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|    3.0|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|    3.0|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|    4.0|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "df.withColumn('new_num', pandas_plus_one(df.num)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Optimus' object has no attribute 'OutlierDetector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-41b1c78401f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Choose a column for analyzing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutlierDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"num\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# With the outliers() method you can use MAD to detect if there is an outlier in your column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# And with the run() method you can see which values are not outliers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Optimus' object has no attribute 'OutlierDetector'"
     ]
    }
   ],
   "source": [
    "# Choose a column for analyzing\n",
    "detector = op.OutlierDetector(df,\"num\")\n",
    "# With the outliers() method you can use MAD to detect if there is an outlier in your column\n",
    "detector.outliers()\n",
    "# And with the run() method you can see which values are not outliers\n",
    "detector.run()\n",
    "# Finally with the delete_outliers() method you can delete existing outliers in your column. \n",
    "# This will modify the dataframe we have used when instantiating the OutlierDetector\n",
    "# (deleting the whole row that contains the outlier value), but the original dataframe that we \n",
    "# read from disk will be intact.\n",
    "detector.delete_outliers().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  2|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  3|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  3|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  4|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#def my_func(attr):\n",
    "#    def inner(df):\n",
    "#        print(attr)\n",
    "#        return F.when(F.col(df)>0 ,1)\n",
    "#    return inner\n",
    "\n",
    "def my_func(col_name, attr):\n",
    "    return F.when(F.col(col_name)>0 ,2)\n",
    "\n",
    "def udf_my_func(value, attr):\n",
    "    return str(value +1)\n",
    "    \n",
    "    \n",
    "def apply(col,func, type = \"columnExp\"):\n",
    "    if type is \"udf\":\n",
    "        def apply_func(attr, func):            \n",
    "            return F.udf(lambda l: func(l, attr))\n",
    "           \n",
    "    else:\n",
    "        def apply_func(attr, func):\n",
    "            def inner(df):\n",
    "                return func(df, attr)\n",
    "            return inner                \n",
    "    \n",
    "    #return df.withColumn(col, func(\"function attrs\")(col))\n",
    "    return df.withColumn(col, apply_func(\"function attrs\", func)(col))\n",
    "    \n",
    "apply(\"num\", my_func).show()\n",
    "apply(\"num\", udf_my_func, \"udf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  2|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  3|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  3|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  4|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def my_func(col_name, attr):\n",
    "    return F.when(F.col(col_name)>0 ,2)\n",
    "\n",
    "def udf_my_func(value, attr):\n",
    "    return str(value +1)\n",
    "    \n",
    "df.cols().apply(\"num\", my_func).show()\n",
    "df.cols().apply(\"num\", udf_my_func, \"udf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|Letter|distances|category|\n",
      "+------+---------+--------+\n",
      "|     A|       20|      20|\n",
      "|     B|       30|      30|\n",
      "|     D|       80|      80|\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(op.get_sc())\n",
    "\n",
    "#sample data\n",
    "a= sqlContext.createDataFrame([(\"A\", 20), (\"B\", 30), (\"D\", 80)],[\"Letter\", \"distances\"])\n",
    "label_list = [\"Great\", \"Good\", \"OK\", \"Please Move\", \"Dead\"]\n",
    "\n",
    "def cate(label, feature_list):\n",
    "\n",
    "    if feature_list == 0:\n",
    "        return label[4]\n",
    "    else:  #you may need to add 'else' condition as well otherwise 'null' will be added in this case\n",
    "        return label\n",
    "\n",
    "def udf_score(label_list):\n",
    "    return udf(lambda l: cate(l, label_list))\n",
    "a.withColumn(\"category\", udf_score(label_list)(col(\"distances\"))).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
