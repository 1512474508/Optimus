{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             ____        __  _                     \n",
      "            / __ \\____  / /_(_)___ ___  __  _______\n",
      "           / / / / __ \\/ __/ / __ `__ \\/ / / / ___/\n",
      "          / /_/ / /_/ / /_/ / / / / / / /_/ (__  ) \n",
      "          \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  \n",
      "              /_/                                  \n",
      "              \n",
      "Just checking that all necessary environments vars are present...\n",
      "-----\n",
      "PYSPARK_PYTHON=python\n",
      "SPARK_HOME=C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\n",
      "JAVA_HOME=C:\\java8\n",
      "-----\n",
      "Starting or getting SparkSession and SparkContext...\n",
      "Setting checkpoint folder (local). If you are in a cluster change it with set_check_point_ folder(path,'hadoop').\n",
      "Deleting previous folder if exists...\n",
      "Creating the checkpoint directory...\n",
      "Done.\n",
      "Optimus successfully imported. Have fun :).\n"
     ]
    }
   ],
   "source": [
    "# Create optimus\n",
    "op = Optimus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe\n",
    "### Spark\n",
    "\n",
    "This is ugly:\n",
    "\n",
    "```\n",
    "val someData = Seq(\n",
    "  Row(8, \"bat\"),\n",
    "  Row(64, \"mouse\"),\n",
    "  Row(-27, \"horse\")\n",
    ")\n",
    "\n",
    "val someSchema = List(\n",
    "  StructField(\"number\", IntegerType, true),\n",
    "  StructField(\"word\", StringType, true)\n",
    ")\n",
    "\n",
    "val someDF = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(someData),\n",
    "  StructType(someSchema)\n",
    ")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thanks Mr Powers\n",
    "df = op.create.df(\n",
    "    [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True),\n",
    "                (\"animals\", \"str\", True),\n",
    "                (\"thing\", StringType(), True),\n",
    "                (\"two strings\", StringType(), True),\n",
    "                (\"filter\", StringType(), True),\n",
    "                (\"num 2\", \"string\", True),\n",
    "                (\"date\", \"string\", True),\n",
    "                (\"num 3\", \"string\", True)\n",
    "                \n",
    "            ],[\n",
    "                (\"  I like     fish  \", 1, \"dog\", \"&^%$#housé\", \"cat-car\", \"a\",\"1\", \"20150510\", \"3\"),\n",
    "                (\"    zombies\", 2, \"cat\", \"tv\", \"dog-tv\", \"b\",\"2\", \"20160510\", \"3\"),\n",
    "                (\"simpsons   cat lady\", 2, \"frog\", \"table\",\"eagle-tv-plus\",\"1\",\"3\", \"20170510\", \"4\"),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"c\",\"4\", \"20180510\", \"5\"),\n",
    "    \n",
    "            ]\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('words', 'string'),\n",
       " ('num', 'int'),\n",
       " ('animals', 'string'),\n",
       " ('thing', 'string'),\n",
       " ('two strings', 'string'),\n",
       " ('filter', 'string'),\n",
       " ('num 2', 'string'),\n",
       " ('date', 'string'),\n",
       " ('num 3', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|new_num|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|      2|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|      3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|      3|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|      4|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "\n",
    "# Input/output are both a single double value\n",
    "@udf('int')\n",
    "def plus_one(v):\n",
    "    return v+1\n",
    "\n",
    "df.withColumn('new_num', plus_one(df.num)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|new_num|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|    2.0|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|    3.0|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|    3.0|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|    4.0|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "df.withColumn('new_num', pandas_plus_one(df.num)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow==0.9.* in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\n",
      "Requirement already satisfied: six>=1.0.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from pyarrow==0.9.*)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from pyarrow==0.9.*)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==0.9.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Optimus' object has no attribute 'OutlierDetector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-41b1c78401f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Choose a column for analyzing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutlierDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"num\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# With the outliers() method you can use MAD to detect if there is an outlier in your column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# And with the run() method you can see which values are not outliers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Optimus' object has no attribute 'OutlierDetector'"
     ]
    }
   ],
   "source": [
    "# Choose a column for analyzing\n",
    "detector = op.OutlierDetector(df,\"num\")\n",
    "# With the outliers() method you can use MAD to detect if there is an outlier in your column\n",
    "detector.outliers()\n",
    "# And with the run() method you can see which values are not outliers\n",
    "detector.run()\n",
    "# Finally with the delete_outliers() method you can delete existing outliers in your column. \n",
    "# This will modify the dataframe we have used when instantiating the OutlierDetector\n",
    "# (deleting the whole row that contains the outlier value), but the original dataframe that we \n",
    "# read from disk will be intact.\n",
    "detector.delete_outliers().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = None\n",
    "if master is None:\n",
    "    master = \"ASdf\"\n",
    "print(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, a=12):\n",
    "        print(a)\n",
    "        pass\n",
    "    \n",
    "    def turnon(self, a= 2, b=4):\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=A()\n",
    "a.turnon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.__all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = op.create.df(\n",
    "    [\n",
    "                (\"num 1\", \"int\", True), # a simple value\n",
    "                (\"num 2\", \"int\", True), # multiple values\n",
    "                (\"num 3\", \"int\", True), # Not values are repeated\n",
    "                \n",
    "            ],[\n",
    "\n",
    "                (3,3,4),\n",
    "                (3,3,3),\n",
    "                (2,2,2),\n",
    "                (1,2,1),\n",
    "                (0,1,0)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "def mode(column):\n",
    "    cnts = df.groupBy(column).count()\n",
    "    mode = cnts.join(\n",
    "        cnts.agg(F.max(\"count\").alias(\"max_\")), F.col(\"count\") == F.col(\"max_\")\n",
    "    )\n",
    "    # if none of the values are repeated    \n",
    "    result =mode.filter(mode[\"count\"]>1).select(column).collect()\n",
    "    values = [i[0] for i in result]\n",
    "    return {column:values}\n",
    "\n",
    "df.cols().apply([\"num 1\"], mode)\n",
    "\n",
    "#print(mode(\"num 1\"))\n",
    "#print (mode(\"num 2\"))\n",
    "#print (mode(\"num 3\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)\n",
    "\n",
    "print([i[0] for i in r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = cnts.join(\n",
    "    cnts.agg(F.max(\"count\").alias(\"max_\")), F.col(\"count\") == F.col(\"max_\")\n",
    ").select(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
