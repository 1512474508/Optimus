{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             ____        __  _                     \n",
      "            / __ \\____  / /_(_)___ ___  __  _______\n",
      "           / / / / __ \\/ __/ / __ `__ \\/ / / / ___/\n",
      "          / /_/ / /_/ / /_/ / / / / / / /_/ (__  ) \n",
      "          \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  \n",
      "              /_/                                  \n",
      "              \n",
      "Just checking that all necessary environments vars are present...\n",
      "-----\n",
      "PYSPARK_PYTHON=python\n",
      "SPARK_HOME=C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\n",
      "JAVA_HOME=C:\\java8\n",
      "-----\n",
      "Starting or getting SparkSession and SparkContext...\n",
      "Setting checkpoint folder ( local ). If you are in a cluster initialize optimus with master='your_ip' as param\n",
      "Deleting previous folder if exists...\n",
      "Creating the checkpoint directory...\n",
      "Optimus successfully imported. Have fun :).\n"
     ]
    }
   ],
   "source": [
    "# Create optimus\n",
    "op = Optimus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe\n",
    "### Spark\n",
    "\n",
    "This is ugly:\n",
    "\n",
    "```\n",
    "val someData = Seq(\n",
    "  Row(8, \"bat\"),\n",
    "  Row(64, \"mouse\"),\n",
    "  Row(-27, \"horse\")\n",
    ")\n",
    "\n",
    "val someSchema = List(\n",
    "  StructField(\"number\", IntegerType, true),\n",
    "  StructField(\"word\", StringType, true)\n",
    ")\n",
    "\n",
    "val someDF = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(someData),\n",
    "  StructType(someSchema)\n",
    ")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thanks Mr Powers\n",
    "df = op.create.df(\n",
    "    [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True),\n",
    "                (\"animals\", \"str\", True),\n",
    "                (\"thing\", StringType(), True),\n",
    "                (\"two strings\", StringType(), True),\n",
    "                (\"filter\", StringType(), True),\n",
    "                (\"num 2\", \"string\", True),\n",
    "                (\"date\", \"string\", True),\n",
    "                (\"num 3\", \"string\", True)\n",
    "                \n",
    "            ],[\n",
    "                (\"  I like     fish  \", 1, \"dog\", \"&^%$#housé\", \"cat-car\", \"a\",\"1\", \"20150510\", \"3\"),\n",
    "                (\"    zombies\", 2, \"cat\", \"tv\", \"dog-tv\", \"b\",\"2\", \"20160510\", \"3\"),\n",
    "                (\"simpsons   cat lady\", 2, \"frog\", \"table\",\"eagle-tv-plus\",\"1\",\"3\", \"20170510\", \"4\"),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"c\",\"4\", \"20180510\", \"5\"),\n",
    "    \n",
    "            ]\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|new_num|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|      2|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|      3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|      3|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|      4|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "\n",
    "# Input/output are both a single double value\n",
    "@udf('int')\n",
    "def plus_one(v):\n",
    "    return v+1\n",
    "\n",
    "df.withColumn('new_num', plus_one(df.num)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|new_num|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|    2.0|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|    3.0|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|    3.0|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|    4.0|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "df.withColumn('new_num', pandas_plus_one(df.num)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Optimus' object has no attribute 'OutlierDetector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-41b1c78401f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Choose a column for analyzing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutlierDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"num\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# With the outliers() method you can use MAD to detect if there is an outlier in your column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# And with the run() method you can see which values are not outliers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Optimus' object has no attribute 'OutlierDetector'"
     ]
    }
   ],
   "source": [
    "# Choose a column for analyzing\n",
    "detector = op.OutlierDetector(df,\"num\")\n",
    "# With the outliers() method you can use MAD to detect if there is an outlier in your column\n",
    "detector.outliers()\n",
    "# And with the run() method you can see which values are not outliers\n",
    "detector.run()\n",
    "# Finally with the delete_outliers() method you can delete existing outliers in your column. \n",
    "# This will modify the dataframe we have used when instantiating the OutlierDetector\n",
    "# (deleting the whole row that contains the outlier value), but the original dataframe that we \n",
    "# read from disk will be intact.\n",
    "detector.delete_outliers().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  2|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  3|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  3|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  4|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#def my_func(attr):\n",
    "#    def inner(df):\n",
    "#        print(attr)\n",
    "#        return F.when(F.col(df)>0 ,1)\n",
    "#    return inner\n",
    "\n",
    "def my_func(col_name, attr):\n",
    "    return F.when(F.col(col_name)>0 ,2)\n",
    "\n",
    "def udf_my_func(value, attr):\n",
    "    return str(value +1)\n",
    "    \n",
    "    \n",
    "def apply(col,func, type = \"columnExp\"):\n",
    "    if type is \"udf\":\n",
    "        def apply_func(attr, func):            \n",
    "            return F.udf(lambda l: func(l, attr))\n",
    "           \n",
    "    else:\n",
    "        def apply_func(attr, func):\n",
    "            def inner(df):\n",
    "                return func(df, attr)\n",
    "            return inner                \n",
    "    \n",
    "    #return df.withColumn(col, func(\"function attrs\")(col))\n",
    "    return df.withColumn(col, apply_func(\"function attrs\", func)(col))\n",
    "    \n",
    "apply(\"num\", my_func).show()\n",
    "apply(\"num\", udf_my_func, \"udf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  2|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  2|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  3|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  3|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  4|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def my_func(col_name, attr):\n",
    "    return F.when(F.col(col_name)>0 ,2)\n",
    "\n",
    "def udf_my_func(value, attr):\n",
    "    return str(value +1)\n",
    "    \n",
    "df.cols().apply(\"num\", my_func).show()\n",
    "df.cols().apply(\"num\", udf_my_func, \"udf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|Letter|distances|category|\n",
      "+------+---------+--------+\n",
      "|     A|       20|      20|\n",
      "|     B|       30|      30|\n",
      "|     D|       80|      80|\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(op.get_sc())\n",
    "\n",
    "#sample data\n",
    "a= sqlContext.createDataFrame([(\"A\", 20), (\"B\", 30), (\"D\", 80)],[\"Letter\", \"distances\"])\n",
    "label_list = [\"Great\", \"Good\", \"OK\", \"Please Move\", \"Dead\"]\n",
    "\n",
    "def cate(label, feature_list):\n",
    "\n",
    "    if feature_list == 0:\n",
    "        return label[4]\n",
    "    else:  #you may need to add 'else' condition as well otherwise 'null' will be added in this case\n",
    "        return label\n",
    "\n",
    "def udf_score(label_list):\n",
    "    return udf(lambda l: cate(l, label_list))\n",
    "a.withColumn(\"category\", udf_score(label_list)(col(\"distances\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_attr =  [('col1','1','3'),('col2','2','4'),('col3','3','5')]\n",
    "cols = [('col1'),('col2'),('col3')]\n",
    "attr = [(1,4),(5,4),(5,6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1 (1, 4)\n",
      "col2 (5, 4)\n",
      "col3 (5, 6)\n"
     ]
    }
   ],
   "source": [
    "cols = ['col1','col2','col3']\n",
    "attr =  [(1,4),(5,4),(5,6)]\n",
    "\n",
    "for i, (a, b) in enumerate(zip(cols, attr)):\n",
    "    print (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['col1', 'col2', 'col3']\n",
      "[('1', '3'), ('2', '4'), ('3', '5')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-f42ef67aef58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0menum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0menum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "columns_attr =  [('col1','1','3'),('col2','2','4'),('col3','3','5')]\n",
    "#columns_attr =  ['col1','col2']\n",
    "def parse(attr):\n",
    "    cols = [(i[0:1][0]) for i in attr]\n",
    "    attrs = [(i[1:]) for i in attr]\n",
    "\n",
    "    return cols, attrs\n",
    "\n",
    "cols, attrs = parse(columns_attr)\n",
    "print(cols)\n",
    "print(attrs)\n",
    "\n",
    "cols = ['num', 'num 2']\n",
    "#attrs =[('double',), ('double',)]\n",
    "attrs = None\n",
    "\n",
    "enum = \n",
    "enum = cols\n",
    "\n",
    "\n",
    "\n",
    "for i, (a, b) in enumerate(zip(cols, attrs)):\n",
    "    print (a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
