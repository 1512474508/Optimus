{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Embedding\n",
    "\n",
    "In 2013, Tomas Mikolov introduced word embedding to learn a better quality of word. At that time word2vec is state of the art on dealing with text. Later on, doc2vec is introduced as well. What if we think in another angel? Instead of aggregate from word to document, is it possible to aggregate from character to word. \n",
    "\n",
    "In this article, you will go through what, why, when and how on Character Embedding\n",
    "\n",
    "**What?**\n",
    "\n",
    "Text Understanding from Scratch introduced using character CNN. In Char CNN paper, a list of character are defined 70 characters which including 26 English letters, 10 digits, 33 special characters and new line character.\n",
    "\n",
    "On the other hand, Google Brain team introduced Exploring the Limits of Language Modeling and released the lm_1b model which includes 256 vectors (including 52 characters, special characters) and the dimension is just 16. By comparing to word embedding, the dimension can increase up to 300 while the number of vectors is huge.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "In english, all words are formed by 26 (or 52 if including both upper and lower case character, or even more if including special characters). Having the character embedding, every single word's vector can be formed even it is out-of-vocabulary words (optional). On the other than, word2vec embedding can only handle those seen words.\n",
    "\n",
    "Another benefit is that it good fits for misspelling words, emoticons, new words (e.g. In 2018, Oxford English Dictionary introduced new word which is boba tea 波霸奶茶. Before that we do not have any pre-trained word embedding for that).\n",
    "It handles infrequent words better than word2vec embedding as later one suffers from lack of enough training opportunity for those rare words.\n",
    "\n",
    "Third reason is that as there are only small amount of vector, it reduces model complexity and improving the performance (in terms of speed)\n",
    "\n",
    "**When?**\n",
    "\n",
    "In NLP, we can apply character embedding on:\n",
    "1. [Text Classification](https://arxiv.org/pdf/1509.01626.pdf)\n",
    "2. [Language Model](https://arxiv.org/pdf/1602.02410.pdf)\n",
    "3. [Named Entity Recognition](https://www.aclweb.org/anthology/Q16-1026)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Dataset**\n",
    "\n",
    "References to news pages collected from an web aggregator in the period from 10-March-2014 to 10-August-2014. The resources are grouped into clusters that represent pages discussing the same story.\n",
    "http://archive.ics.uci.edu/ml/datasets/News+Aggregator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original notebook https://github.com/makcedward/nlp/blob/master/sample/nlp-character_embedding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions flow **\n",
    "\n",
    "* preprocess()\n",
    "    * build_char_dictionary()\n",
    "    * convert_labels()\n",
    "    \n",
    "* process()\n",
    "    * transform_raw_data()\n",
    "        * sent_tokenize\n",
    "\t* transform_training_data()\n",
    "\t\n",
    "* build_model()\n",
    "\t* build_sentence_block()\n",
    "        * Input\n",
    "        * Embedding        \n",
    "        * _build_character_block\n",
    "        * concatenate\n",
    "        * Droput\n",
    "        * Model\n",
    "\t* build_document_block()\n",
    "        * Input - input\n",
    "        * TimeDistributed - output\n",
    "        * Bidirectional - LTSM\n",
    "        * Dropout\n",
    "        * Dense\n",
    "        * Dropout\n",
    "        * Dense\n",
    "        * Model\n",
    "        * compile\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\argenisleon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    # Get file from http://archive.ics.uci.edu/ml/datasets/News+Aggregator\n",
    "    df = pd.read_csv(\n",
    "        \"data/newsCorpora.csv\", \n",
    "        sep='\\t', \n",
    "        names=['id', 'headline', 'url', 'publisher', 'category', 'story', 'hostname', 'timestamp']\n",
    "    )\n",
    "    \n",
    "    # Category: b = business, t = science and technology, e = entertainment, m = health\n",
    "    return df[['category', 'headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                           headline\n",
       "0        b  Fed official says weak data caused by weather,...\n",
       "1        b  Fed's Charles Plosser sees high bar for change...\n",
       "2        b  US open: Stocks fall after Fed official hints ...\n",
       "3        b  Fed risks falling 'behind the curve', Charles ...\n",
       "4        b  Fed's Plosser: Nasty Weather Has Curbed Job Gr..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_raw_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news: 422419\n"
     ]
    }
   ],
   "source": [
    "print('Number of news: %d' % (len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "**_Character Embedding_**\n",
    "1. Define a list a characters (i.e. m). For example, can use alphanumeric and some special characters. For my example, English characters (52), number (10), special characters (20) and one unknown character, UNK. Total 83 characters.\n",
    "2. Transfer characters as 1-hot encoding and got a sequence for vectors. For unknown characters and blank characters, use all-zero vector to replace it. If exceeding pre-defined maximum length of characters (i.e. l), ignoring it. The output is 16 dimensions vector per every single character.\n",
    "3. Using 3 1D CNN layers (configurable) to learn the sequence\n",
    "\n",
    "**_Sentence Embedding_**\n",
    "1. Bi-directional LSTM followed CNN layers\n",
    "2. Some dropout layers are added after LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function flow\n",
    "\n",
    "preprocess\n",
    "\tbuild_char_dictionary\n",
    "\tconvert_labels\n",
    "process\n",
    "\t_transform_raw_data\n",
    "\t_transform_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from charcnn import CharCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Stage: preprocess\n",
      "5\n",
      "['b' 't' 'e' 'm']\n",
      "Totoal number of chars: 83\n",
      "First 3 char_indices sample: {'UNK': 0, '%': 1, 'b': 2}\n",
      "First 3 indices_char sample: {0: 'UNK', 1: '%', 2: 'b'}\n",
      "Label to Index:  {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
      "Index to Label:  {0: 'b', 1: 't', 2: 'e', 3: 'm'}\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of characters per sentence is 256.\n",
    "# Maximum number of sentence is 5\n",
    "\n",
    "char_cnn = CharCNN(max_len_of_sentence=256, max_num_of_setnence=5, verbose = 6)\n",
    "\n",
    "#First of all, we need to prepare meta information including character dictionary \n",
    "#and converting label from text to numeric (as keras support numeric input only).\n",
    "\n",
    "char_cnn.preprocess(labels=df['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category                                           headline\n",
      "224652        m        Dramatic increase in MERS cases since March\n",
      "325223        e  In Talks: Frank Darabont to Helm 'The Huntsman...\n",
      "27519         e  Gisele Bundchen and Tom Brady selling estate i...\n",
      "84787         t  Facebook could be working on a Secret-inspired...\n",
      "43298         e  CinemaCon: The New Trailer for Hercules, Starr...\n",
      "128382        b  Chipotle raising prices as steak, avocados, ch...\n",
      "266060        m   Eva Longoria's Las Vegas Strip steakhouse closes\n",
      "392455        t  Best new mobile apps for iOS and Android: Top ...\n",
      "417912        b  US Index Futures Little Changed After S&P 500 ...\n",
      "26494         e  President Obama to promote Obamacare on 'Ellen...\n",
      "374287        b         Fed plans to end taper in October: minutes\n",
      "285713        b            Pay raises depend on industry you're in\n",
      "142030        e  Coachella goes corporate, as marketers can't r...\n",
      "181404        t  White House top climate threats: California an...\n",
      "274731        e  Casting Net: Will Smith may tackle NFL concuss...\n",
      "294691        t                  Gasoline prices continue to go up\n",
      "204609        t  Apple to Beat Competitors in Streaming Music w...\n",
      "309108        t           US stocks close below records; FMC falls\n",
      "420703        e  Discovery Channel officially canceled Sons of ...\n",
      "46658         b  Fed bars shareholder payouts from Citi, four o...\n",
      "422256        m                   Ebola outbreak 'could top 20000'\n",
      "103816        e  Mt. Pleasant First UMC announces Holy Week ser...\n",
      "101756        b  Amazon offers workers $2000 to quit, but the i...\n",
      "194917        e  Jennifer Lawrence Reveals How She Makes It Wor...\n",
      "413764        t  Defective iPhone 5 Batteries Will Get Free Rep...\n",
      "50441         b  US economy grew at 2.6 per cent rate in 4Q as ...\n",
      "381155        e  Watch Weird Al Yankovic parody Robin Thicke's ...\n",
      "139495        t  NASA's LADEE Spacecraft Crashes on Dark Side o...\n",
      "270353        e  Jonah Hill Apologizes For Dropping Homophobic ...\n",
      "152674        t  Team Win Brings Their Latest Recovery To The G...\n",
      "...         ...                                                ...\n",
      "359130        b              Why PetSmart, Inc. Stock Jumped Today\n",
      "127321        m  11-year-old girl mauled by raccoon as a baby t...\n",
      "160264        t  Storyful joins with Facebook to launch FB News...\n",
      "151577        m  Lincoln County won't exempt water bought for w...\n",
      "82765         e  Toby Kebbell cast as Fantastic Four's Doctor Doom\n",
      "218896        b  Consumers and experts weigh in on AT&T-DirecTV...\n",
      "192119        b  'Candy Crush' maker King serves up bittersweet...\n",
      "8479          b  Business Digest: Stocks finish mostly higher a...\n",
      "276397        b       Senate confirms two Federal Reserve nominees\n",
      "160417        t  President Obama plays football with Asimo robo...\n",
      "43962         b  MH370: US law firm launches legal action again...\n",
      "104480        m                  A misguided 'feel good' statement\n",
      "187358        e  Recap: Game of Thrones, S4, E5 - First of His ...\n",
      "94186         e    Police: Legendary star Mickey Rooney dies at 93\n",
      "61399         t  Instagram surpasses '200 million active users'...\n",
      "61812         t  VIDEO: Mozilla Employees Ask CEO To Step Down ...\n",
      "96620         b         Walmart will stock Wild Oats organic foods\n",
      "283181        t  TweetDeck outage due to to 'heart' symbol: Bug...\n",
      "208757        t  'Moto G Cinema' name leaked by Motorola's website\n",
      "72630         e  New Michael Jackson Album 'Xscape' Coming This...\n",
      "256984        e  Journey's Steve Perry takes stage for 1st time...\n",
      "281907        b  US STOCKS-Futures imply weak open, S&P on trac...\n",
      "181679        t      MS Office No Longer Essential at Work: Report\n",
      "233216        e              Spotify tops ten million paying users\n",
      "418224        b      CBO Report: Deficit Widens With Slower Growth\n",
      "106274        e                 Johnny Depp In Murder Case Scandal\n",
      "172871        b      Eurozone inflation up in April, Eurostat says\n",
      "313396        e         Lana Del Rey gets DOWN and DIRTY with fans\n",
      "359033        m  Despite recall, some in Modesto not staying aw...\n",
      "416422        e  'Breaking Bad,' 'Modern Family' big winners at...\n",
      "\n",
      "[337935 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173373</th>\n",
       "      <td>b</td>\n",
       "      <td>Alstom Studies GE Offer, Leaves Door Open for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84543</th>\n",
       "      <td>t</td>\n",
       "      <td>Google preparing Android TV launch? Leaked det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238202</th>\n",
       "      <td>b</td>\n",
       "      <td>Temporary Fee On Big Businesses Funds Obamacare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                           headline\n",
       "173373        b  Alstom Studies GE Offer, Leaves Door Open for ...\n",
       "84543         t  Google preparing Android TV launch? Leaked det...\n",
       "238202        b    Temporary Fee On Big Businesses Funds Obamacare"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df)\n",
    "#train_df.iloc[2]\n",
    "train_df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Stage: process\n",
      "Number of news: 10\n",
      "Actual max sentence: 1\n",
      "{'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
      "Shape:  (10, 5, 256) (10,)\n"
     ]
    }
   ],
   "source": [
    "df_sample = train_df.sample(10)\n",
    "\n",
    "x_train, y_train = char_cnn.process(\n",
    "    df=df_sample, x_col='headline', y_col='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183622</th>\n",
       "      <td>m</td>\n",
       "      <td>International public health emergency: Polio s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353037</th>\n",
       "      <td>b</td>\n",
       "      <td>How Will PACCAR (PCAR) Stock Respond To Volksw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409844</th>\n",
       "      <td>b</td>\n",
       "      <td>India's Factory Output Reaches 17-Month High: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359972</th>\n",
       "      <td>t</td>\n",
       "      <td>U.S. regulators should just ban premium SMS pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316913</th>\n",
       "      <td>b</td>\n",
       "      <td>Sector Update: Energy Stocks Rising; Pioneer N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159827</th>\n",
       "      <td>t</td>\n",
       "      <td>Facebook Buys Fitness App to Track Your 'Moves'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286376</th>\n",
       "      <td>t</td>\n",
       "      <td>Destiny's extended E3 trailer does a good job ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152035</th>\n",
       "      <td>b</td>\n",
       "      <td>TheStreet Downgrades Coca-Cola Bottling Co. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291930</th>\n",
       "      <td>e</td>\n",
       "      <td>'Game of Thrones' Season 4 Finale Made More Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217441</th>\n",
       "      <td>t</td>\n",
       "      <td>First Camelopardalid meteor shower expected ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                           headline\n",
       "183622        m  International public health emergency: Polio s...\n",
       "353037        b  How Will PACCAR (PCAR) Stock Respond To Volksw...\n",
       "409844        b  India's Factory Output Reaches 17-Month High: ...\n",
       "359972        t  U.S. regulators should just ban premium SMS pr...\n",
       "316913        b  Sector Update: Energy Stocks Rising; Pioneer N...\n",
       "159827        t    Facebook Buys Fitness App to Track Your 'Moves'\n",
       "286376        t  Destiny's extended E3 trailer does a good job ...\n",
       "152035        b  TheStreet Downgrades Coca-Cola Bottling Co. Co...\n",
       "291930        e  'Game of Thrones' Season 4 Finale Made More Sh...\n",
       "217441        t  First Camelopardalid meteor shower expected ne..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "[[ 0  0  0 ... 47 39 33]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "[3 0 0 1 0 1 1 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "#  y_train can be done with and string indexer function. its convert  every posible lable to a integer representation. \n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Stage: process\n",
      "Number of news: 337935\n",
      "Actual max sentence: 15\n",
      "{'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
      "Shape:  (337935, 5, 256) (337935,)\n"
     ]
    }
   ],
   "source": [
    "# We have to transform raw input training data and testing to numpy format for keras input\n",
    "\n",
    "x_train, y_train = char_cnn.process(\n",
    "    df=train_df, x_col='headline', y_col='category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 50 42 41\n",
      " 17 35  5 37 42 39 53  7  5  7 37  7 17 42  5 58 14  6 35  5 39 53  5 37\n",
      "  7 17 37 41 42 39 53  5 42 53 47 17 25 17 41 79]\n"
     ]
    }
   ],
   "source": [
    "#print(train_df)\n",
    "print(x_train[0][0])\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = char_cnn.process(\n",
    "    df=test_df, x_col='headline', y_col='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Stage: build model\n",
      "-----> Stage: train model\n",
      "Train on 337935 samples, validate on 84484 samples\n",
      "Epoch 1/10\n",
      " 12288/337935 [>.............................] - ETA: 26:44:09 - loss: 1.3712 - acc: 0.3313"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4ef918c6a7b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mchar_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mchar_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mchar_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./char_cnn_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\nlp\\sample\\charcnn.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, y_train, x_test, y_test, batch_size, epochs, shuffle)\u001b[0m\n\u001b[0;32m    272\u001b[0m         self.get_model().fit(\n\u001b[0;32m    273\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             batch_size=batch_size, epochs=epochs, shuffle=shuffle)\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;31m#         return self.model['doc_encoder']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "char_cnn.build_model()\n",
    "char_cnn.train(x_train, y_train, x_test, y_test, batch_size=2048, epochs=10)\n",
    "\n",
    "char_cnn.get_model().save('./char_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model_loaded = load_model('./char_cnn_model.h5')\n",
    "char_cnn_model_loaded.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Character Embedding is a brilliant design for solving lots of text classification. It resolved some word embedding. FAIR did a further step. They introduced to use subword embedding to build fastText. \n",
    "\n",
    "This is some comment on Character Embedding as it does not include any word meaning but just using characters. We may include both Character Embedding and Word Embedding together to solve our NLP problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
