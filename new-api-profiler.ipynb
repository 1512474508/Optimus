{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             ____        __  _                     \n",
      "            / __ \\____  / /_(_)___ ___  __  _______\n",
      "           / / / / __ \\/ __/ / __ `__ \\/ / / / ___/\n",
      "          / /_/ / /_/ / /_/ / / / / / / /_/ (__  ) \n",
      "          \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  \n",
      "              /_/                                  \n",
      "              \n",
      "Just checking that all necessary environments vars are present...\n",
      "-----\n",
      "PYSPARK_PYTHON=python\n",
      "SPARK_HOME=C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\n",
      "JAVA_HOME=C:\\java8\n",
      "-----\n",
      "Starting or getting SparkSession and SparkContext...\n",
      "Setting checkpoint folder ( local ). If you are in a cluster initialize optimus with master='your_ip' as param\n",
      "Deleting previous folder if exists...\n",
      "Creating the checkpoint directory...\n",
      "Optimus successfully imported. Have fun :).\n"
     ]
    }
   ],
   "source": [
    "# Create optimus\n",
    "op = Optimus(master=\"local\", app_name= \"optimus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = op.read.csv(\"Meteorite_Landings.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------+------------+--------+----+--------------------+----------+-----------+--------------------+\n",
      "|               name| id|nametype|    recclass|mass (g)|fall|                year|    reclat|    reclong|         GeoLocation|\n",
      "+-------------------+---+--------+------------+--------+----+--------------------+----------+-----------+--------------------+\n",
      "|             Aachen|  1|   Valid|          L5|      21|Fell|01/01/1880 12:00:...| 50.775000|   6.083330|(50.775000, 6.083...|\n",
      "|             Aarhus|  2|   Valid|          H6|     720|Fell|01/01/1951 12:00:...| 56.183330|  10.233330|(56.183330, 10.23...|\n",
      "|               Abee|  6|   Valid|         EH4|  107000|Fell|01/01/1952 12:00:...| 54.216670|-113.000000|(54.216670, -113....|\n",
      "|           Acapulco| 10|   Valid| Acapulcoite|    1914|Fell|01/01/1976 12:00:...| 16.883330| -99.900000|(16.883330, -99.9...|\n",
      "|            Achiras|370|   Valid|          L6|     780|Fell|01/01/1902 12:00:...|-33.166670| -64.950000|(-33.166670, -64....|\n",
      "|           Adhi Kot|379|   Valid|         EH4|    4239|Fell|01/01/1919 12:00:...| 32.100000|  71.800000|(32.100000, 71.80...|\n",
      "|Adzhi-Bogdo (stone)|390|   Valid|       LL3-6|     910|Fell|01/01/1949 12:00:...| 44.833330|  95.166670|(44.833330, 95.16...|\n",
      "|               Agen|392|   Valid|          H5|   30000|Fell|01/01/1814 12:00:...| 44.216670|   0.616670|(44.216670, 0.616...|\n",
      "|             Aguada|398|   Valid|          L6|    1620|Fell|01/01/1930 12:00:...|-31.600000| -65.233330|(-31.600000, -65....|\n",
      "|      Aguila Blanca|417|   Valid|           L|    1440|Fell|01/01/1920 12:00:...|-30.866670| -64.550000|(-30.866670, -64....|\n",
      "|   Aioun el Atrouss|423|   Valid|Diogenite-pm|    1000|Fell|01/01/1974 12:00:...| 16.398060|  -9.570280|(16.398060, -9.57...|\n",
      "|                Aïr|424|   Valid|          L6|   24000|Fell|01/01/1925 12:00:...| 19.083330|   8.383330|(19.083330, 8.383...|\n",
      "|    Aire-sur-la-Lys|425|   Valid|     Unknown|    null|Fell|01/01/1769 12:00:...| 50.666670|   2.333330|(50.666670, 2.333...|\n",
      "|              Akaba|426|   Valid|          L6|     779|Fell|01/01/1949 12:00:...| 29.516670|  35.050000|(29.516670, 35.05...|\n",
      "|           Akbarpur|427|   Valid|          H4|    1800|Fell|01/01/1838 12:00:...| 29.716670|  77.950000|(29.716670, 77.95...|\n",
      "|            Akwanga|432|   Valid|           H|    3000|Fell|01/01/1959 12:00:...|  8.916670|   8.433330|(8.916670, 8.433330)|\n",
      "|            Akyumak|433|   Valid|   Iron, IVA|   50000|Fell|01/01/1981 12:00:...| 39.916670|  42.816670|(39.916670, 42.81...|\n",
      "|            Al Rais|446|   Valid|      CR2-an|     160|Fell|01/01/1957 12:00:...| 24.416670|  39.516670|(24.416670, 39.51...|\n",
      "|          Al Zarnkh|447|   Valid|         LL5|     700|Fell|01/01/2001 12:00:...| 13.660330|  28.960000|(13.660330, 28.96...|\n",
      "|              Alais|448|   Valid|         CI1|    6000|Fell|01/01/1806 12:00:...| 44.116670|   4.083330|(44.116670, 4.083...|\n",
      "+-------------------+---+--------+------------+--------+----+--------------------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType, ArrayType\n",
    "df = op.create.df(\n",
    "            [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True),\n",
    "                (\"animals\", \"str\", True),\n",
    "                (\"thing\", StringType(), True),\n",
    "                (\"two strings\", StringType(), True),\n",
    "                (\"filter\", StringType(), True),\n",
    "                (\"num 2\", \"string\", True),\n",
    "                (\"bool\", \"string\", True)\n",
    "\n",
    "            ]\n",
    ",\n",
    "[\n",
    "                (\"  I like     fish  \", 1, \"dog\", \"housé\", \"cat-car\", \"a\",\"1\", \"True\"),\n",
    "                (\"    zombies\", 2, \"cat\", \"tv\", \"dog-tv\", \"b\",\"2\", \"True\"),\n",
    "                (\"simpsons   cat lady\", 2, \"frog\", \"table\",\"eagle-tv-plus\",\"1\",\"3\", \"False\"),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"c\",\"4\", \"False\"),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"True\",1, False)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+-----+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2| bool|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1| True|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2| True|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|False|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|False|\n",
      "|               null|  3|  eagle|glass|      lion-pc|  True|    1|false|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler = op.profiler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': {'filter': {'uniques_count': 5,\n",
       "   'p_uniques': 100.0,\n",
       "   'missing_count': 0,\n",
       "   'p_missing': 0.0,\n",
       "   'frequency': [{'value': 3, 'count': 2, 'percentage': 40.0},\n",
       "    {'value': 2, 'count': 2, 'percentage': 40.0},\n",
       "    {'value': 1, 'count': 1, 'percentage': 20.0}]},\n",
       "  'num': {'uniques_count': 3,\n",
       "   'p_uniques': 60.0,\n",
       "   'missing_count': 0,\n",
       "   'p_missing': 0.0,\n",
       "   'frequency': [{'value': 3, 'count': 2, 'percentage': 40.0},\n",
       "    {'value': 2, 'count': 2, 'percentage': 40.0},\n",
       "    {'value': 1, 'count': 1, 'percentage': 20.0}]}},\n",
       " 'rows_count': 5}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler.columns([\"filter\",\"num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.dataset_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.count_data_types('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.columns_by_types('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.cols().cast([(\"reclat\", \"integer\"),(\"reclong\",\"integer\")])\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"reclat\", outputCol=\"reclat_features\")\n",
    "bucketedData = bucketizer.transform(df)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
