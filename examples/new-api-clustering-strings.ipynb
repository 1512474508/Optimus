{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importing sql types\n",
    "from pyspark.sql.types import *\n",
    "# Importing sql functions\n",
    "from pyspark.sql.functions import *\n",
    "# Importing plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "# Importing numeric module\n",
    "import numpy as np\n",
    "# Importing local dataFrame library\n",
    "import pandas as pd\n",
    "# Importing features to build tables\n",
    "from IPython.display import display, HTML\n",
    "# Importing time librarie to measure time\n",
    "import time\n",
    "from functools import partial\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from pyspark import SQLContext; \n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This class makes an analisis of dataframe datatypes and its different general features.\n",
    "class DataFrameTransformer():\n",
    "    def __init__(self, df):\n",
    "        # Dataframe\n",
    "        self.df = df\n",
    "    \n",
    "    # Funtion to analize column datatypes, it also plot proportion datatypes and histograms:   \n",
    "    # Transformation functions:\n",
    "    # Trimming spaces in columns:\n",
    "    def trimCol(self, columns):\n",
    "        # Function to trim spaces in columns with strings datatype\n",
    "        def colTrim(columns):\n",
    "            exprs = [trim(col(c)).alias(c) if c in columns else c for (c, t) in self.df.dtypes]\n",
    "            self.df = self.df.select(*exprs)\n",
    "\n",
    "        # Asserting data variable first is integer:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "        \n",
    "        # If None or [] is provided with column parameter:\n",
    "        if (columns == \"*\"): columns = validCols\n",
    "        \n",
    "        # Columns \n",
    "        if type(columns) == type('s'): columns = [columns]\n",
    "        \n",
    "        assert all(col in validCols for col in columns), 'Error: Columns or column does not exist in the dataFrame or or is numberic column'\n",
    "        \n",
    "        # Trimming spaces in columns:\n",
    "        colTrim(columns)\n",
    "\n",
    "    # Drop\n",
    "    def dropCol(self, columns):\n",
    "        \n",
    "        def colDrop(columns):\n",
    "            exprs = filter(lambda c: c not in columns, self.df.columns)\n",
    "            self.df = self.df.select(*exprs)\n",
    "    \n",
    "        # Asserting data variable is string or list:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "    \n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert columns != [], \"Error: Column can not be a empty list []\"\n",
    "        \n",
    "        # Columns \n",
    "        if type(columns) == type('s'): columns = [columns]\n",
    "            \n",
    "        # Columns \n",
    "        assert all(col in self.df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "        \n",
    "        # Calling colDrop function\n",
    "        colDrop(columns)\n",
    "        \n",
    "    def replaceCol(self, search, changeTo, columns):\n",
    "        \n",
    "        def colReplace(columns):\n",
    "            self.df = self.df.replace(search, changeTo, subset=columns)\n",
    "            \n",
    "        \n",
    "        # Asserting columns is string or list:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "        # Asserting search parameter is a string or a number\n",
    "        assert type(search) == type(' ') or type(search) == type(1.2) or type(search) == type(1), \"Error: Search parameter must be a number or string\"\n",
    "        \n",
    "        # Asserting changeTo parameter is a string or a number\n",
    "        assert type(changeTo) == type(' ') or type(changeTo) == type(1.2) or type(changeTo) == type(1), \"Error: changeTo parameter must be a number or string\"\n",
    "        \n",
    "        # Asserting search and changeTo have same type\n",
    "        assert type(search) == type(changeTo), 'Error: Search and ChangeTo must have same datatype: Integer, String, Float'\n",
    "        \n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert columns != [], \"Error: Column can not be a empty list []\"\n",
    "        \n",
    "        # Change \n",
    "        types = {type(''): 'string', type(int(1)): 'int', type(float(1.2)): 'float', type(1.2): 'double'};\n",
    "               \n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == types[type(search)], self.df.dtypes)]     \n",
    "        \n",
    "        # If None or [] is provided with column parameter:\n",
    "        if (columns == \"*\"): columns = validCols[:]\n",
    "            \n",
    "        # If columns is string, make a list:\n",
    "        if type(columns) == type(' '): columns = [columns]\n",
    "            \n",
    "        # Asserting if columns provided are in dataFrame:\n",
    "        assert all(col in self.df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "   \n",
    "        # Asserting if selected column datatype and search and changeTo parameters are the same:\n",
    "        assert all(column in validCols for column in  columns), 'Error: Column and Search and ChangeTo do not have same datatype'\n",
    "        \n",
    "        colReplace(columns)\n",
    "        \n",
    "    def dropRow(self, columns):\n",
    "            # Function to trim spaces in columns with strings datatype\n",
    "        def rowDrop(columns):\n",
    "            exprs = [trim(col(c)).alias(c) if c in columns else c for (c, t) in self.df.dtypes]\n",
    "            self.df = self.df.select(*exprs)\n",
    "\n",
    "        # Asserting data variable column is string or column:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "        \n",
    "        # If None or [] is provided with column parameter:\n",
    "        if (columns == \"*\"): columns = validCols\n",
    "        \n",
    "        # Columns \n",
    "        if type(columns) == type('s'): columns = [columns]\n",
    "        \n",
    "        assert all(col in validCols for col in columns), 'Error: Columns or column does not exist in the dataFrame or or is numberic column'\n",
    "        \n",
    "        # Trimming spaces in columns:\n",
    "        colTrim(columns)\n",
    "        \n",
    "    def deleteRow(self, func):\n",
    "        self.df = self.df.filter(func)\n",
    "    \n",
    "        #tempFunc = lambda inColumn, inSearch: (inColumn > inSearch);  \n",
    "        #delectRow(df, tempFunc(col('integers'), 7))\n",
    "        \n",
    "        #tempFunc = lambda inColumn, inSearchMin, inSearchMax: (inColumn > inSearchMin) & (inColumn < inSearchMax);  \n",
    "        #deleteRow(df, tempFunc(col('integers'), 7, '9.0'))\n",
    "\n",
    "    def setCol(self, columns, func, dataType):\n",
    "\n",
    "        dictTypes = {'string': StringType(), 'str': StringType(), 'integer': IntegerType(), \n",
    "                     'int': IntegerType(), 'float': FloatType(), 'double': DoubleType(), 'Double': DoubleType()}\n",
    "\n",
    "        Types = {'string': 'string', 'str': 'string', 'String': 'string','integer': 'int', \n",
    "                     'int': 'int', 'float': 'float', 'double': 'double', 'Double': 'double'}\n",
    "\n",
    "        try:\n",
    "            function = udf(func, dictTypes[dataType])\n",
    "        except KeyError:       \n",
    "            assert False, \"Error, dataType not recognized\"\n",
    "\n",
    "        def colSet(columns ,function):\n",
    "            exprs = [function(col(c)).alias(c) if c in columns else c for (c, t) in self.df.dtypes]\n",
    "            try:\n",
    "                self.df = self.df.select(*exprs)\n",
    "            except:\n",
    "                assert False, \"Error: Make sure if operation is compatible with row datatype.\"\n",
    "\n",
    "        # Asserting data variable column is string or column:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == Types[dataType], self.df.dtypes)]\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        if (columns == \"*\"): columns = validCols[:]\n",
    "\n",
    "        # If columns is string, make a list:\n",
    "        if type(columns) == type(' '): columns = [columns]\n",
    "\n",
    "        # Asserting if columns provided are in dataFrame:\n",
    "        assert all(col in self.df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "\n",
    "        # Asserting if selected column datatype and search and changeTo parameters are the same:\n",
    "        assert all(column in validCols for column in  columns), 'Error: Column and datatype parameters do not have same datatype'\n",
    "\n",
    "        colSet(columns, function)\n",
    "    \n",
    "    # Drop\n",
    "    def keepCol(self, columns):\n",
    "        \n",
    "        def colKeep(columns):\n",
    "            exprs = filter(lambda c: c in columns, self.df.columns)\n",
    "            self.df = self.df.select(*exprs)\n",
    "    \n",
    "        # Asserting data variable is string or list:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "    \n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert columns != [], \"Error: Column can not be a empty list []\"\n",
    "        \n",
    "        # Columns \n",
    "        if type(columns) == type('s'): columns = [columns]\n",
    "            \n",
    "        # Columns \n",
    "        assert all(col in self.df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "        \n",
    "        # Calling colDrop function\n",
    "        colKeep(columns)\n",
    "        \n",
    "    def clearAccents(self, columns):\n",
    "        # Asserting columns is string or list:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "        \n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert columns != [], \"Error: Column can not be a empty list []\"\n",
    "        \n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        if (columns == \"*\"): columns = validCols[:]\n",
    "            \n",
    "        # If columns is string, make a list:\n",
    "        if type(columns) == type(' '): columns = [columns]\n",
    "        \n",
    "        # Asserting if columns provided are in dataFrame:\n",
    "        assert all(col in self.df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "\n",
    "        # Asserting if selected column datatype and search and changeTo parameters are the same:\n",
    "        assert all(column in validCols for column in  columns), 'Error: Column and datatype parameters do not have same datatype'\n",
    "\n",
    "        \n",
    "        # Receives  a string as an argument\n",
    "        def remove_accents(inputStr):\n",
    "            # first, normalize strings:\n",
    "            nfkdStr = unicodedata.normalize('NFKD', inputStr)\n",
    "            # Keep chars that has no other char combined (i.e. accents chars)\n",
    "            withOutAccents = u\"\".join([c for c in nfkdStr if not unicodedata.combining(c)])\n",
    "            return withOutAccents\n",
    "\n",
    "        function = udf(lambda x: remove_accents(x), StringType())\n",
    "        exprs = [function(col(c)).alias(c) if c in columns else c for c in self.df.columns]\n",
    "        self.df = self.df.select(*exprs)\n",
    "\n",
    "        \n",
    "    def removeSpecialChars(self, columns):\n",
    "        # Asserting columns is string or list:\n",
    "        assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "        \n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert columns != [], \"Error: Column can not be a empty list []\"\n",
    "         \n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        if (columns == \"*\"): columns = validCols[:]\n",
    "            \n",
    "        # If columns is string, make a list:\n",
    "        if type(columns) == type(' '): columns = [columns]\n",
    "        \n",
    "        # Asserting if columns provided are in dataFrame:\n",
    "        assert all(col in self.df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "\n",
    "        # Asserting if selected column datatype and search and changeTo parameters are the same:\n",
    "        assert all(column in validCols for column in  columns), 'Error: Column and datatype parameters do not have same datatype'\n",
    "\n",
    "        def rmSpecChars(inputStr):\n",
    "            #Remove all punctuation and control characters\n",
    "            for punct in (set(inputStr) & set(string.punctuation)):\n",
    "                inputStr = inputStr.replace(punct,\"\")\n",
    "            return inputStr\n",
    "\n",
    "        # User define function that does operation in cells\n",
    "        function = udf(lambda cell: rmSpecChars(cell), StringType())\n",
    "\n",
    "        exprs = [function(col(c)).alias(c) if c in columns else c for c in self.df.columns]\n",
    "        self.df = self.df.select(*exprs)\n",
    "        \n",
    "    def renameCol(self, column, newName):\n",
    "        # Asserting columns is string or list:\n",
    "        assert type(column) == type(''), \"Error: Column argument must be a string\"\n",
    "       \n",
    "        # Asserting columns is string or list:\n",
    "        assert type(newName) == type(''), \"Error: New column name argument must be a string or a list.\"\n",
    "       \n",
    "        # Asserting parameters are not empty strings:\n",
    "        assert ((column != '') and (newName != '')), \"Error: Input parameters can't be empty strings\"\n",
    "       \n",
    "        # Asserting input column is in column of dataFrame:\n",
    "        self.df = df.withColumnRenamed(column, newName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Si se desea un data set corto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------+-----------+-------------------+\n",
      "|          strings|integers|integers2|     floats|             double|\n",
      "+-----------------+--------+---------+-----------+-------------------+\n",
      "|            Pizza|       1|        2| 0.27276418| 0.6613167128437065|\n",
      "|           Piz;za|       7|        2| 0.11393974| 0.6985988822235742|\n",
      "|            pízzá|       8|        6|  0.6150019|0.49384417423959026|\n",
      "|            pizza|       5|        6| 0.68132496| 0.3598785838606483|\n",
      "|            pizza|       3|        4|  0.1834308| 0.2925823602461035|\n",
      "|            Pizza|       4|        2|  0.5925784|0.43374931785012694|\n",
      "|            Pizza|       1|        8|  0.3665534| 0.9427310133178698|\n",
      "|    pizza!       |       3|        9| 0.61260915|0.07517403736806638|\n",
      "|    pizza!       |       5|        9| 0.67193794|0.47009181351076623|\n",
      "|           Piz;za|       9|        7| 0.42232946|0.08115945367243682|\n",
      "|            piZza|       6|        5| 0.33166534|0.29004299937090117|\n",
      "|    pizza!       |       3|        6| 0.15858446|  0.764976073072821|\n",
      "|    pizza!       |       4|        5| 0.63457966| 0.8281012098047101|\n",
      "|            piZza|       6|        2|  0.3107597| 0.8471970906071973|\n",
      "|           Piz;za|       9|        4|  0.5648889| 0.5402400516598852|\n",
      "|            piZza|       7|        0|0.024864897|0.06890750608862029|\n",
      "|           Piz;za|       3|        8|  0.5661604| 0.6217557168108709|\n",
      "|           PIZZA;|       9|        5|  0.8479246| 0.4028045975356953|\n",
      "|            pízzá|       3|        7| 0.56584847| 0.3630904132089394|\n",
      "|            pizza|       0|        7|  0.8461415| 0.4588043992769698|\n",
      "+-----------------+--------+---------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Creación de un dataframe de prueba:\n",
    "schema = StructType(\n",
    "        [\n",
    "        StructField(\"strings\", StringType(), True),\n",
    "         StructField(\"integers\", IntegerType(), True),\n",
    "        #StructField(\"booleans\",  BooleanType(), True),\n",
    "        StructField(\"integers2\", IntegerType(), True),\n",
    "        StructField(\"floats\",  FloatType(), True),\n",
    "        StructField(\"double\",  DoubleType(), True)\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Cambiando una columna de strings en double\n",
    "#df = df.withColumn('pepa', df.pepa + 0)\n",
    "\n",
    "size = 200\n",
    "# Generating strings column:\n",
    "foods = ['    pizza!       ', 'pizza', 'PIZZA;', 'piZza', 'pízzá', 'Pizza', 'Piz;za']\n",
    "foods = [foods[random.randint(0,6)] for count in range(size)]\n",
    "# Generating integer column:\n",
    "numCol1 = [random.randint(0,9) for number in range(size)]\n",
    "# Generating boolean column:\n",
    "#numCol2 = [bool(random.getrandbits(1)) for number in range(size)]\n",
    "numCol2 = [random.randint(0,9) for number in range(size)]\n",
    "# Generating integer column:\n",
    "numCol3 = [random.random() for number in range(size)]\n",
    "# Generating integer column:\n",
    "numCol4 = [random.random() for number in range(size)]\n",
    "\n",
    "# Building dataFrame:\n",
    "df = sqlContext.createDataFrame(list(zip(foods, numCol1, numCol2, numCol3, numCol4)),schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el analizador, con el df de entrada:\n",
    "analizer = DataFrameTransformer(df)\n",
    "# Trim:\n",
    "#analizer.trimCol(columns=\"*\")\n",
    "# Drop:\n",
    "#analizer.dropCol(columns=['integers2'])\n",
    "# Replace:\n",
    "#analizer.replaceCol(1,1544544,\"*\")\n",
    "# DeleteRow:\n",
    "#tempFunc = lambda inColumn, inSearchMin, inSearchMax: (inColumn > inSearchMin) & (inColumn < inSearchMax);  \n",
    "#deleteRow(df, tempFunc(col('integers'), 7, '9.0'))\n",
    "#tempFunc = lambda inColumn, inSearch: (inColumn > inSearch);  \n",
    "#analizer.deleteRow(tempFunc(col('integers'), 7))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df = df.withColumn('Price', df.Price + 0)\n",
    "\n",
    "#funcion = lambda cell: cell + 10045400 if (cell > 100000 ) else cell\n",
    "#analizer.setCol(['Price'], funcion, 'double')\n",
    "\n",
    "#analizer.df.select('Price').show()\n",
    "#analizer.keepCol(['strings','integers'])\n",
    "#analizer.df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#a = Analizer.DataFrameAnalizer(df, sc)\n",
    "#a.columnAnalize(columnList=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (' Replacing a number if value in cell is greater than 5:')\n",
    "# Replacing a number:   \n",
    "slen = udf(lambda cell: 10000 if (cell > 5 ) else cell, IntegerType())\n",
    "setCol(df, ['integers'], slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number operation:\n",
    "print (' Add a number to cells in columns:')\n",
    "slen = udf(lambda cell: cell + 10000 , FloatType())\n",
    "setCol(df, ['floats'], slen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String manipulation:\n",
    "print ('Uppercase operation in column:')\n",
    "slen = udf(lambda cell: cell.upper(), StringType())\n",
    "setCol(df, ['strings'], slen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Si se desea un data set real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters:\n",
    "#pathFile = '/datasets/quiero-casa-simplificado-arg-sexo.csv' \n",
    "#delimiter_mark = ','\n",
    "\n",
    "# filename = \"/datasets/quierocasa-2.csv\"\n",
    "#delimiter_mark = ';'\n",
    "\n",
    "filename = \"file:///home/andrea/Downloads/QuieroCasa.csv\"\n",
    "\n",
    "#filename = 'file:///home/hreyes/pythoncleaning/cleaning-quiero-casa/datasets/world-development-indicators/Indicators.csv'\n",
    "#delimiter_mark = ','\n",
    "#pathFile = filename\n",
    "\n",
    "#pathFile = 'file:///home/hreyes/pythoncleaning/cleaning-quiero-casa/datasets/prueba'\n",
    "delimiter_mark = ';'\n",
    "#filename = 'file:///home/hreyes/pythoncleaning/cleaning-quiero-casa/datasets/quierocasa.csv'\n",
    "\n",
    "\n",
    "# Reading data:\n",
    "df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .options(delimiter=delimiter_mark)\\\n",
    "    .options(inferSchema='true')\\\n",
    "    .load(filename) \\\n",
    "    .cache()\n",
    "\n",
    "# Reading data:\n",
    "#df = sqlContext.read.parquet(pathFile)\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frecuencia de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|           STATE|count|\n",
      "+----------------+-----+\n",
      "|Distriro Federal|  259|\n",
      "|Estado de México|  810|\n",
      "|     México D.F.| 2383|\n",
      "|            D.F.|   66|\n",
      "|     Mexico D.F.|  112|\n",
      "|Distrito Federal|11523|\n",
      "|DISTRITO FEDERAL|  339|\n",
      "|distrito federal|   68|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "def fq(df, columns, sort_by_count = True):\n",
    "    def frecuency(columns, sort_by_count):\n",
    "        \n",
    "        if sort_by_count:\n",
    "            for column in columns:\n",
    "                freq = (df\n",
    "                        .groupBy(column)\n",
    "                        .count()\n",
    "                        .orderBy(\"count\", ascending=False))\n",
    "\n",
    "                if (freq.where(\"count > 1\").count() == 0):\n",
    "                    print(\"No existen valores que agrupar en la columna\", column)\n",
    "                else:\n",
    "                    freq.show()\n",
    "        else:\n",
    "            for column in columns:\n",
    "                freq = (df\n",
    "                        .groupBy(column)\n",
    "                        .count())\n",
    "\n",
    "                if (freq.where(\"count > 1\").count() == 0):\n",
    "                    print(\"No existen valores que agrupar en la columna\", column)\n",
    "                else:\n",
    "                    freq.show()\n",
    "\n",
    "    # Asserting data variable is string or list:\n",
    "    assert type(columns) == type('s') or type(columns) == type([]), \"Error: Column argument must be a string or a list.\"\n",
    "\n",
    "    # If None or [] is provided with column parameter:\n",
    "    assert columns != [], \"Error: Column can not be a empty list []\"\n",
    "\n",
    "    # Columns \n",
    "    if type(columns) == type('s'): columns = [columns]\n",
    "\n",
    "    # Columns \n",
    "    assert all(col in df.columns for col in columns), 'Error: Columns or column does not exist in the dataFrame'\n",
    "\n",
    "    frecuency(columns, sort_by_count)\n",
    "    \n",
    "def getFreq(df, column, sort_by_count = True):\n",
    "    def frecuency(column, sort_by_count):\n",
    "\n",
    "        if sort_by_count:\n",
    "            freq = (df\n",
    "                    .groupBy(column)\n",
    "                    .count()\n",
    "                    .orderBy(\"count\", ascending=False))\n",
    "        else:\n",
    "            freq = (df\n",
    "                    .groupBy(column)\n",
    "                    .count())\n",
    "            \n",
    "        if (freq.where(\"count > 1\").count() == 0):\n",
    "            print(\"No existen valores que agrupar en la columna\", column)\n",
    "            return sqlContext.createDataFrame(sc.emptyRDD(), StructType([]))\n",
    "        else:\n",
    "            return freq\n",
    "\n",
    "    # Asserting data variable is string:\n",
    "    assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "\n",
    "    # Columns \n",
    "    assert column in df.columns, 'Error: Column does not exist in the dataFrame'\n",
    "    \n",
    "    return frecuency(column, sort_by_count)\n",
    "\n",
    "fq(df, [\"STATE\"], False)\n",
    "# getFreq(df, \"STATE\", False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering based on Key Collision (Fingerprint and N-Gram Fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from functools import reduce\n",
    "\n",
    "class KeyCollision():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    \"\"\"\n",
    "    Cluster a dataframe column based on the Fingerprint algorithm\n",
    "    \"\"\"\n",
    "    def __cluster_fingerprints__(self, column, outputCol, sort_tokens, remove_duplicates):\n",
    "        def removeAndSort(string, sort_tokens, remove_duplicates):\n",
    "            split_key = string.split()\n",
    "\n",
    "            # remove duplicates, if chosen\n",
    "            if remove_duplicates:\n",
    "                dups_removed = set(split_key)\n",
    "            else:\n",
    "                dups_removed = split_key\n",
    "\n",
    "            # sort the tokens, if chosen\n",
    "            if sort_tokens:\n",
    "                # sort the tokens\n",
    "                sorted_split_key = sorted(dups_removed)\n",
    "            else:\n",
    "                sorted_split_key = dups_removed\n",
    "\n",
    "            # join the tokens back together\n",
    "            return \" \".join(sorted_split_key)\n",
    "        \n",
    "        fCol = outputCol\n",
    "        clusters = (self.df\n",
    "                   .groupBy(column)\n",
    "                   .count()\n",
    "                   .select('count', column)\n",
    "                   .withColumn(fCol, self.df[column])\n",
    "                   .cache())\n",
    "\n",
    "        \n",
    "#         remove leading and trailing whitespace\n",
    "#         change all characters to their lowercase representation\n",
    "#         remove all punctuation and control characters\n",
    "#         split the string into whitespace-separated tokens\n",
    "#         sort the tokens and remove duplicates\n",
    "#         join the tokens back together\n",
    "\n",
    "        exprs = [trim(col(c)).alias(c) if c == fCol else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "        \n",
    "        exprs = [lower(col(c)).alias(c) if c == fCol else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "        \n",
    "        def rmvSpecialChars(inputStr):\n",
    "            for punct in (set(inputStr) & set(string.punctuation)):\n",
    "                inputStr = inputStr.replace(punct,\"\")\n",
    "            return inputStr\n",
    "            \n",
    "        func = udf(lambda cell: rmvSpecialChars(cell), StringType())\n",
    "        exprs = [func(col(c)).alias(c) if c == fCol else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "\n",
    "        removeAndSortFunc = lambda cell: removeAndSort(cell, sort_tokens, remove_duplicates)\n",
    "        func = udf(removeAndSortFunc, StringType())\n",
    "        exprs = [func(col(c)).alias(c) if c == fCol else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "        \n",
    "        clusters.unpersist()\n",
    "        return clusters\n",
    "    \n",
    "    \"\"\"\n",
    "    Cluster a DataFrame column based on the N-Gram Fingerprint algorithm\n",
    "    \"\"\"\n",
    "    def __cluster_ngram__(self, column, n_size, outputCol):\n",
    "        def joinUnique(seq):\n",
    "            seen = set()\n",
    "            x = [x for x in seq if not (x in seen or seen.add(x))]\n",
    "            return \"\".join(x).replace(' ', '')\n",
    "        \n",
    "        newColumn = column + \"Changes\"\n",
    "        nGramCol = outputCol\n",
    "        \n",
    "        clusters = (self.df.select(column)\n",
    "                   .groupBy(column)\n",
    "                   .count()\n",
    "                   .withColumn(newColumn, self.df[column])\n",
    "                   .cache())\n",
    "        \n",
    "#/         change all characters to their lowercase representation\n",
    "#/         remove all punctuation, whitespace, and control characters\n",
    "#         obtain all the string n-grams\n",
    "#         sort the n-grams and remove duplicates\n",
    "#         join the sorted n-grams back together\n",
    "\n",
    "        exprs = [lower(col(c)).alias(c) if c == newColumn else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "        \n",
    "        def rmvSpecialChars(inputStr):\n",
    "            for punct in (set(inputStr) & set(string.punctuation)):\n",
    "                inputStr = inputStr.replace(punct,\"\")\n",
    "            return \"\".join(inputStr.split())\n",
    "            \n",
    "        func = udf(lambda cell: rmvSpecialChars(cell), StringType())\n",
    "        exprs = [func(col(c)).alias(c) if c == newColumn else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "        \n",
    "        exprs = [trim(col(c)).alias(c) if c == newColumn else c for c in clusters.columns]\n",
    "        clusters = clusters.select(*exprs)\n",
    "                \n",
    "        # create NGram object\n",
    "        ngram = NGram(n=n_size, inputCol=newColumn, outputCol=nGramCol)\n",
    "        \n",
    "        # flatten newColumn string values and ngram transform the column\n",
    "        exprs = [split(col(c), '').alias(c) if c == newColumn else c for c in clusters.columns]\n",
    "        ngram_df = ngram.transform(clusters.select(*exprs))\n",
    "        \n",
    "        # sort and remove duplicate ngrams\n",
    "        clusters = (ngram_df\n",
    "                    .select(*['count', column, sort_array(nGramCol)\n",
    "                              .alias(nGramCol)]))\n",
    "        \n",
    "        # join sorted, unique ngrams back together\n",
    "        function = udf(joinUnique, StringType())\n",
    "        exprs = [function(col(c)).alias(c) if c == nGramCol else c for c in clusters.columns]\n",
    "        clusters= (clusters.select(*exprs))\n",
    "        \n",
    "        clusters.unpersist()\n",
    "        return clusters\n",
    "        \n",
    "    \"\"\"\n",
    "    Returns a clustered dataframe column as a dataframe with columns [strings = String, fingerprints = String]\n",
    "    \"\"\"\n",
    "    def fingerprint(self, column, outputCol = \"Fingerprint\", sort_tokens=False, remove_duplicates=False):\n",
    "        # Asserting data variable is string or list:\n",
    "        assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert column != \"\", \"Error: Column can not be a empty string\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        # asserts the column parameter is a string column of the dataFrame\n",
    "        assert column in validCols, 'Error: Columns or column does not exist in the dataFrame or is numberic column'       \n",
    "\n",
    "        c = self.__cluster_fingerprints__(column, outputCol, sort_tokens, remove_duplicates)\n",
    "        return c\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a clustered dataframe column as a dataframe with columns [strings = String, nGrams = String]\n",
    "    \"\"\"\n",
    "    def nGramFingerprint(self, column, n_size=2, outputCol=\"nGram\"):\n",
    "        # Asserting data variable is string or list:\n",
    "        assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert column != \"\", \"Error: Column can not be a empty string\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        # asserts the column parameter is a string column of the dataFrame\n",
    "        assert column in validCols, 'Error: Columns or column does not exist in the dataFrame or is numberic column'       \n",
    "\n",
    "        c = self.__cluster_ngram__(column, n_size, outputCol)\n",
    "        return c\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the clustered dataframe column as a Python list to provide iterative properties\n",
    "    \"\"\"\n",
    "    def fingerprintList(self, column, outputCol = 'Fingerprint', sort_tokens=False, remove_duplicates=False):\n",
    "        def list_by_fingerprint(column, outputCol, sort_tokens, remove_duplicates):\n",
    "            clusterDF = self.__cluster_fingerprints__(column, outputCol, sort_tokens, remove_duplicates)\n",
    "            \n",
    "#             (clusterDF.select('*')\n",
    "#                 .join(clusterDF\n",
    "#                       .groupBy('fingerprint')\n",
    "#                       .agg(max('count')\n",
    "#                           .alias('count'))\n",
    "#                       , ['count', 'fingerprint']))\n",
    "            \n",
    "            # crea un mapa con el formato: {ngram: [raw strings]}\n",
    "\n",
    "            def ext(a, b): return a + [b]\n",
    "            def listify(a): return [a]\n",
    "            \n",
    "            return (clusterDF.map(lambda r: r[outputCol])\n",
    "                       .zip(clusterDF.map(lambda f: f[column]))\n",
    "                       .combineByKey(listify, ext, ext)\n",
    "                       .collectAsMap())\n",
    "    \n",
    "        # Asserting data variable is string or list:\n",
    "        assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert column != \"\", \"Error: Column can not be a empty string\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        # asserts the column parameter is a string column of the dataFrame\n",
    "        assert column in validCols, 'Error: Columns or column does not exist in the dataFrame or is numberic column'       \n",
    "\n",
    "        c = list_by_fingerprint(column, outputCol, sort_tokens, remove_duplicates)\n",
    "        return c\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the clustered dataframe column as a Python list to provide iterative properties\n",
    "    \"\"\"\n",
    "    def nGramFingerprintList(self, column, n_size=2, outputCol='nGram'):\n",
    "        def list_by_fingerprint(column, n_size, outputCol):\n",
    "            clusterDF = self.__cluster_ngram__(column, n_size, outputCol)\n",
    "            \n",
    "            # crea un mapa con el formato: {ngram: [raw strings]}\n",
    "\n",
    "            def ext(a, b): return a + [b]\n",
    "            def listify(a): return [a]\n",
    "            \n",
    "            return (clusterDF.map(lambda r: r[outputCol])\n",
    "                       .zip(clusterDF.map(lambda f: f[column]))\n",
    "                       .combineByKey(listify, ext, ext)\n",
    "                       .collectAsMap())\n",
    "    \n",
    "        # Asserting data variable is string or list:\n",
    "        assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "\n",
    "        # If None or [] is provided with column parameter:\n",
    "        assert column != \"\", \"Error: Column can not be a empty string\"\n",
    "\n",
    "        # Filters all string columns in dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        # asserts the column parameter is a string column of the dataFrame\n",
    "        assert column in validCols, 'Error: Columns or column does not exist in the dataFrame or is numberic column'  \n",
    "        \n",
    "        c = list_by_fingerprint(column, n_size, outputCol)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+----------------+\n",
      "|count|           STATE|     Fingerprint|\n",
      "+-----+----------------+----------------+\n",
      "|  259|Distriro Federal|distriro federal|\n",
      "|  810|Estado de México|estado de méxico|\n",
      "| 2383|     México D.F.|       méxico df|\n",
      "|   66|            D.F.|              df|\n",
      "|  112|     Mexico D.F.|       mexico df|\n",
      "|11523|Distrito Federal|distrito federal|\n",
      "|  339|DISTRITO FEDERAL|distrito federal|\n",
      "|   68|distrito federal|distrito federal|\n",
      "+-----+----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'': [''],\n",
       " 'abajo con eleva autos': ['Abajo con eleva autos'],\n",
       " 'abajo sin eleva autos': ['Abajo sin eleva autos'],\n",
       " 'adelante': ['Adelante'],\n",
       " 'adelante doble': ['Adelante doble', 'adelante doble'],\n",
       " 'adelante simple': ['Adelante simple'],\n",
       " 'adelante triple': ['Adelante triple', 'Adelante Triple', 'adelante triple'],\n",
       " 'arriba con eleva autos': ['Arriba con eleva autos'],\n",
       " 'atrás doble': ['Atrás doble', 'atrás doble'],\n",
       " 'atrás simple': ['Atrás simple'],\n",
       " 'atrás triple': ['Atrás triple', 'atrás triple'],\n",
       " 'cajon virtual': ['Cajon virtual', 'cajon virtual', 'Cajon Virtual'],\n",
       " 'en medio': ['En medio'],\n",
       " 'en medio simple': ['En medio simple'],\n",
       " 'en medio triple': ['En medio triple', 'En medio Triple', 'en medio triple'],\n",
       " 'único': ['Único', 'único', 'ÚNICO']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = KeyCollision(df)\n",
    "test.fingerprint('STATE').show()\n",
    "test.fingerprintList('UBICAHORI')\n",
    "# dict(test.fingerprintList('UBICAHORI'))\n",
    "# test.nGramFingerprint('STATE', 1).show()\n",
    "# print(test.nGramFingerprintList('STATE', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection based on MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import floor\n",
    "from pyspark.sql.functions import col, abs\n",
    "import time\n",
    "\n",
    "class OutlierDetector():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        \n",
    "    def median(self, df, column):\n",
    "        \n",
    "        # Asserting column is string:\n",
    "        assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "        # Asserting if column provided is in dataFrame:\n",
    "        assert column in self.df.columns, 'Error: Column does not exist in the dataFrame'\n",
    "        \n",
    "        p=0.5\n",
    "\n",
    "        n = df.count()\n",
    "        h = (n - 1) * p\n",
    "        \n",
    "        df.registerTempTable(\"table\")\n",
    "\n",
    "        return (sqlContext\n",
    "                .sql(\"SELECT \\\n",
    "                        ROUND( \\\n",
    "                        PERCENTILE_APPROX(\" + column + \", 0.5), 2) \\\n",
    "                        AS \" + column + \" FROM table\")\n",
    "                .map(lambda x: x[column])\n",
    "                .first())\n",
    "        \n",
    "    def mad(self, column, sample=None, seed=None):\n",
    "        \"\"\"\n",
    "        Computes median absolute deviation for the numeric column 'column' in the dataframe 'df'\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        \n",
    "        # Asserting column is string:\n",
    "        assert type(column) == type('s'), \"Error: Column argument must be a string.\"\n",
    "        # Asserting if column provided is in dataFrame:\n",
    "        assert column in self.df.columns, 'Error: Column does not exist in the dataFrame'\n",
    "\n",
    "        assert sample is None or 0 < sample <= 1\n",
    "\n",
    "        seed = seed if seed is not None else time.time()\n",
    "        self.df = self.df if sample is None else df.sample(False, sample, seed)\n",
    "\n",
    "        # MAD = median of the absolute deviations from the median\n",
    "        # steps:\n",
    "        # 1) calculate median \n",
    "        # 2) calculate difference of all values from the median\n",
    "        # 3) create rdd containing the absolute deviation \n",
    "        # 3) calculate median of this rdd\n",
    "\n",
    "        self.medianValue = self.median(self.df, column)\n",
    "\n",
    "        absoluteDeviation = (self.df\n",
    "                             .select(column)\n",
    "                             .orderBy(column)\n",
    "                             .withColumn(column, abs(col(column) - self.medianValue))\n",
    "                             .cache())\n",
    "\n",
    "        self.madValue = self.median(absoluteDeviation, column)\n",
    "        \n",
    "        return self.madValue\n",
    "        \n",
    "        \n",
    "    def rangeLimits(self, threshold=2):\n",
    "        \"\"\"\n",
    "        Returns an array with the limits of the range of accepted values, that is,\n",
    "        the range given by [median-constant*MAD, median+constant*MAD]\n",
    "        \"\"\"\n",
    "        \n",
    "        if not(hasattr(self, 'column')):\n",
    "            # if mad hasn't been calculated\n",
    "            assert False, (\"Cannot get values within range if MAD value has not been computed.\\n\" \n",
    "                           \"Please run OutlierDetector.mad(column, sample=None, seed=None)\")\n",
    "\n",
    "        self.limits = []\n",
    "        self.limits.append(round((self.medianValue - threshold*self.madValue), 2))\n",
    "        self.limits.append(round((self.medianValue + threshold*self.madValue), 2))\n",
    "        \n",
    "        return self.limits\n",
    "\n",
    "    def run(self, threshold=2):\n",
    "        \"\"\"\n",
    "        Get list of values within accepted range, without duplicates\n",
    "        \"\"\"\n",
    "        \n",
    "        if not(hasattr(self, 'column')):\n",
    "            # if mad hasn't been calculated\n",
    "            assert False, (\"Cannot get values within range if MAD value has not been computed.\\n\" \n",
    "                           \"Please run OutlierDetector.mad(column, sample=None, seed=None)\")\n",
    "        else:\n",
    "            if not(hasattr(self, 'limits')):\n",
    "                # if limits have not been calculated but mad has\n",
    "                self.rangeLimits(threshold)\n",
    "                \n",
    "            \n",
    "        limits = self.limits\n",
    "        column = self.column\n",
    "\n",
    "        self.valuesWithinRange = list(set((self.df\n",
    "                                           .map(lambda x: x[column])\n",
    "                                           .filter(lambda x: x>=limits[0] and x<=limits[1])\n",
    "                                           .collect())))\n",
    "        \n",
    "        return self.valuesWithinRange\n",
    "    \n",
    "    \n",
    "    def outliers(self, threshold=2):\n",
    "        \"\"\"\n",
    "        Get list of values within accepted range, without duplicates\n",
    "        \"\"\"\n",
    "        \n",
    "        if not(hasattr(self, 'column')):\n",
    "            # if mad hasn't been calculated\n",
    "            assert False, (\"Cannot get values within range if MAD value has not been computed.\\n\" \n",
    "                           \"Please run OutlierDetector.mad(column, sample=None, seed=None)\")\n",
    "        else:\n",
    "            if not(hasattr(self, 'limits')):\n",
    "                # if limits have not been calculated but mad has\n",
    "                self.rangeLimits(threshold)\n",
    "                \n",
    "            \n",
    "        limits = self.limits\n",
    "        column = self.column\n",
    "\n",
    "        self.valuesWithoutRange = list(set((self.df\n",
    "                                            .map(lambda x: x[column])\n",
    "                                            .filter(lambda x: x<limits[0] or x>limits[1])\n",
    "                                            .collect())))\n",
    "        \n",
    "        return self.valuesWithoutRange\n",
    "\n",
    "    def deleteOutliers(self, threshold=2):\n",
    "        \"\"\"\n",
    "        Deletes all rows where values in the column are outliers\n",
    "        \"\"\"\n",
    "        \n",
    "        if not(hasattr(self, 'column')):\n",
    "            # if mad hasn't been calculated\n",
    "            assert False, (\"Cannot get values within range if MAD value has not been computed.\\n\" \n",
    "                           \"Please run OutlierDetector.mad(column, sample=None, seed=None)\")\n",
    "        else:\n",
    "            if not(hasattr(self, 'limits')):\n",
    "                # if limits have not been calculated but mad has\n",
    "                self.rangeLimits(threshold)\n",
    "            \n",
    "        limits = self.limits\n",
    "        column = self.column\n",
    "        \n",
    "        func = lambda x: (x>=limits[0]) & (x<=limits[1])\n",
    "        self.df = self.df.filter(func(col(column)))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def replaceOutliers(self, replacement, dataType, threshold=2):\n",
    "        \"\"\"\n",
    "        Replaces outliers value with 'replacement' of a specified 'dataType' (string)\n",
    "        \"\"\"\n",
    "        \n",
    "        dictTypes = {'string': StringType(), 'str': StringType(), 'StringType()': StringType(), \n",
    "                     'integer': IntegerType(), 'int': IntegerType(), 'IntegerType()': IntegerType(),\n",
    "                     'float': FloatType(), 'FloatType()': FloatType(),\n",
    "                     'double': DoubleType(), 'Double': DoubleType(), 'DoubleType()': DoubleType()}\n",
    "        \n",
    "        if not(hasattr(self, 'column')):\n",
    "            # if mad hasn't been calculated\n",
    "            assert False, (\"Cannot get values within range if MAD value has not been computed.\\n\" \n",
    "                           \"Please run OutlierDetector.mad(column, sample=None, seed=None)\")\n",
    "        else:\n",
    "            if not(hasattr(self, 'limits')):\n",
    "                # if limits have not been calculated but mad has\n",
    "                self.rangeLimits(threshold)\n",
    "            \n",
    "        limits = self.limits\n",
    "        column = self.column\n",
    "        \n",
    "        func = udf(lambda x: replacement if (x<limits[0]) | (x>limits[1]) else x,\n",
    "                  dictTypes[dataType])\n",
    "        exprs = [func(col(c)).alias(c) if c is column else c for c in self.df.columns]\n",
    "        self.df = self.df.select(*exprs)\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median:  6820.0\n",
      "Median absolute deviation:  2700.0\n",
      "\n",
      "Range limits:  [1420.0, 12220.0]\n"
     ]
    }
   ],
   "source": [
    "detector = OutlierDetector(df)\n",
    "# print(\"Original dataframe (unchanged)\")\n",
    "# df.show()\n",
    "median = detector.median(df, 'ZIPCODE')\n",
    "# median = detector.median(df, 'double')\n",
    "print(\"\\nMedian: \", median)\n",
    "print(\"Median absolute deviation: \", detector.mad('ZIPCODE'))\n",
    "print(\"\\nRange limits: \", detector.rangeLimits(2))\n",
    "# print(\"\\nValues within accepted range: \", detector.run())\n",
    "# print(\"\\nOutlier values: \", detector.outliers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|ZIPCODE|\n",
      "+-------+\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "|   7730|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector.replaceOutliers(0, 'int')\n",
    "detector.df.select('ZIPCODE').show()\n",
    "# Rows with col = 0 would be deleted if OutlierDetector.deleteOutliers() would've been run instead.\n",
    "# A suggested option for a real user is to use the output of OutlierDetector.median(df, column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering based on Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "class Levenshtein():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs clustering on a string column 'column' based on the levenshtein distance\n",
    "    between all values\n",
    "    \"\"\"\n",
    "    def cluster_leven(self, column, distance):\n",
    "        keyer = KeyCollision(self.df)\n",
    "        \n",
    "        first_col = 'fingerprint'\n",
    "        second_col = '_fingerprint_'\n",
    "        distance_col = 'levenshtein'\n",
    "        \n",
    "        fdf = keyer.fingerprint(column, first_col)\n",
    "        fdict = dict(keyer.fingerprintList(column, first_col))\n",
    "        \n",
    "        fingerprints = (fdf.select(first_col)\n",
    "                        .distinct()\n",
    "                        .cache())\n",
    "        \n",
    "        fingerprint_count = (fdf\n",
    "                              .groupBy('fingerprint')\n",
    "                              .agg(sum('count')\n",
    "                                  .alias('count')))\n",
    "        \n",
    "        # computes cartesian product and get combinations between values\n",
    "        # and obtains a distance matrix based on the levenshtein distance\n",
    "       \n",
    "        condition = first_col + '!=' + second_col\n",
    "        l_matrix = (fingerprints.select(first_col)\n",
    "                    .join(fingerprints.select(fingerprints[first_col].alias(second_col)), how='outer')\n",
    "                    .distinct()\n",
    "                    .filter(condition)\n",
    "                    .withColumn('levenshtein', levenshtein(first_col, second_col))\n",
    "                    .cache())\n",
    "        \n",
    "        clusters = {} # dict to contain clusters\n",
    "        cid = 0       # indexed cluster id for iterating over clusters\n",
    "        pair = []     # pair of elements with levenshtein < distance\n",
    "        smallest = 0  # smallest distance found in the DF\n",
    "        \n",
    "        # get the smallest distance (as an int) present in the matrix \n",
    "        smallest = l_matrix.map(lambda d: d[distance_col]).min()\n",
    "\n",
    "        # cluster procedure, clustering will be perfomed over rows with levenshtein < distance\n",
    "        while (smallest < distance):\n",
    "            \n",
    "            grouped = False # flag to determine if pair has been added to a cluster\n",
    "\n",
    "            # retrieves the first combination of strings with the smalles distance as a list\n",
    "            first = l_matrix.filter(l_matrix[distance_col] == smallest).first()\n",
    "            pair = [first[0], first[1]]\n",
    "\n",
    "            # iterate over existing clusters\n",
    "            for cid in clusters:\n",
    "                \n",
    "                # if any of the items in pair is in any of the clusters, \n",
    "                # the other item should be added to the list\n",
    "                if any(raw in clusters[cid] for fingerprint in pair for raw in fdict[fingerprint]):\n",
    "                    clusters[cid].extend(raw \n",
    "                                         for fingerprint in pair \n",
    "                                         for raw in fdict[fingerprint] \n",
    "                                         if raw not in clusters[cid])\n",
    "                    grouped = True\n",
    "                    break;\n",
    "\n",
    "            # if pair was not added to a cluster, create a new one and add both elements\n",
    "            if not grouped:\n",
    "                cid = len(clusters)\n",
    "                clusters[cid] = []\n",
    "                clusters[cid].extend(raw \n",
    "                                     for fingerprint in pair \n",
    "                                     for raw in fdict[fingerprint])                 \n",
    "                    \n",
    "            # index of item with smallest 'count' on fingerprint_count\n",
    "            dropped = 1\n",
    "            \n",
    "            # filter to remove all columns that include the value that will be dropped\n",
    "            l_matrix = (l_matrix\n",
    "                        .filter(l_matrix[first_col] != pair[dropped])\n",
    "                        .filter(l_matrix[second_col] != pair[dropped]))\n",
    "            \n",
    "            smallest = l_matrix.map(lambda d: d[distance_col]).min()\n",
    "            \n",
    "        l_matrix.unpersist()\n",
    "        fingerprints.unpersist()\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leven = Levenshtein(df)\n",
    "c = leven.cluster_leven('STATE', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['México D.F.', 'Mexico D.F.', 'D.F.'], 1: ['Distriro Federal', 'Distrito Federal', 'DISTRITO FEDERAL', 'distrito federal']}\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
