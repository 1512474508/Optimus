{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas comparision\n",
    "Pandas vs Spark\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific funcitons for data cleaning like * for column selection or apply operation over multiple columns reneme or cast\n",
    "A coherent API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very good presentation about pandas and Spark https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas\n",
    "\n",
    "## Pandas and Spark\n",
    "\n",
    "|Description|Pandas|Spark\n",
    "|---|---|---|\n",
    "|Load CSV|read_csv()|read_csv()|\n",
    "|View Dataframe|df.show()|df.head()|\n",
    "|Columns|df.columns|df.columns|\n",
    "|Data Types|df.dtypes()|df.dtypes()|\n",
    "|Rename Columns|df.columns = ['a','b','c'] | df.toDF('a','b','c')|\n",
    "|Drop Column|df.drop()|df.drop()|\n",
    "|Filtering|df[df.mpg < 20]|df[df.mpg < 20]|\n",
    "|Add column |df['gpm']=1/df.mpg|df.withColumn('gpm', 1/df.mpg)|\n",
    "|Fill Nulls|df.fillna(0)|df.fillna(0)|\n",
    "|Aggregation| df.groupby(['cyl','gear']).agg({'mpg':'mean', 'disp':'min'})) |df.groupby(['cyl','gear']).agg({'mpg':'mean', 'disp':'min'}))|\n",
    "\n",
    "* apply you need to declare a udf \n",
    "* left and merge.\n",
    "\n",
    "* statistics\n",
    "* describe\n",
    "* hist\n",
    "\n",
    "https://twitter.com/collegeisfun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimus\n",
    "\n",
    "## Pandas and Optimus side by side\n",
    "\n",
    "From the web:\n",
    "\n",
    "Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. \n",
    "\n",
    "Here is list of the 90th percentil functions used in pandas. Thanks to Devin Petersohn. https://rise.cs.berkeley.edu/blog/pandas-on-ray-early-lessons/\n",
    "\n",
    "|Description|Pandas|Spark|Optimus\n",
    "|---|---|---|---|\n",
    "|Read csv file|pd.read_csv()|spark.read.csv()|op.read.csv()|\n",
    "|Create Dataframe|pd.Dataframe|df.createdataframe()|op.create.df()|\n",
    "|Append Row|df.append|df.union()|df.row().append()|\n",
    "|Column Mean|df.mean|done via agg|df.cols().mean()|\n",
    "|Show Rows from Dataframe|df.head()|df.show()|df.show()|\n",
    "|Drop Columns|df.drop()|df.drop()|df.cols().drop()\n",
    "|Sum all values in a Column|df.sum()|done via agg function|df.cols().sum()|\n",
    "|Save Dataframe to csv|df.to_csv()|df.save().csv()|\n",
    "||df.get()|NI|\n",
    "|Get the mode of a column|df.mode()|df.cols().mode()|\n",
    "|Cast a Column|df.astype()|df.cols().cast(),  astype() as alias|\n",
    "|Substract 2 dataframes|df.sub()|NI|\n",
    "|Merge to dataframes|pd.concat()|optimus.concat()|\n",
    "|Apply a user defined fucntion to a column|df.apply|df.cols().apply()|\n",
    "|Group rows|df.groupby()|df.groupby()|\n",
    "|Joint operation between to dataframes|df.join()|df.join()|\n",
    "|Fill Null values with x|df.fillna()|df.fillna()|\n",
    "|Get the max number of a Column|df.max()|df.cols().max()|\n",
    "|Reset index|reset_index|NA|\n",
    "\n",
    "NI= Not implemented\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimus\n",
    "\n",
    "## Pandas, Optimus and Spark side by side\n",
    "\n",
    "|Pandas|Optimus|Spark|\n",
    "|---|---|---|\n",
    "|pd.read_csv()|op.read.csv()|spark.read.csv()|\n",
    "|pd.Dataframe|op.create.df()|df.createdataframe()|\n",
    "|pd.append|df.row().append()|df.union()|\n",
    "|pd.mean|df.cols().mean()|done via agg|\n",
    "|df.head()|df.head()|df.show()|\n",
    "|df.drop()|df.cols().drop()|df.drop()|\n",
    "|df.sum()|df.cols().sum()|done via agg function|\n",
    "|df.to_csv()|df.save().csv()|df.read.csv()|\n",
    "|df.get()|NI|NI|\n",
    "|df.mode()|df.cols().mode()|done via agg function|\n",
    "|df.astype()|df.cols().cast(),  astype() as alias|df.cast()|\n",
    "|df.sub()|NI|NI|\n",
    "|pd.concat()|optimus.concat()|NI|\n",
    "|df.apply|df.cols().apply()|NI|\t\t\t\n",
    "|df.groupby()|via Spark DataFrame|df.groupby()|\n",
    "|df.join()|via Spark DataFrame|df.join()|\n",
    "|df.fillna()|via Spark DataFrame|df.fillna()|\n",
    "|df.max()|df.cols().max()|done via agg function|\n",
    "|reset_index|NA|NA|\n",
    "\n",
    "NI= Not implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A beautifull way to work with data\n",
    "\n",
    "This is an API proposal to access data.\n",
    "\n",
    "Dataframes would have rows and columns. \n",
    "\n",
    "* To access columns just use df.cols()\n",
    "* To access rows just use df.rows()\n",
    "* If you want to load and save data or concat dataframes just use op.read.csv(). op.save.csv() and op.concat()\n",
    "\n",
    "\n",
    "Easy and simple\n",
    "\n",
    "We identify some ome basic operations for rows and columns:\n",
    "* Create\n",
    "* Select\n",
    "* Between Elements(Columns or Rows)\n",
    "* In Element\n",
    "* Delete\n",
    "\n",
    "\n",
    "\n",
    "### Operations \n",
    "* Columns\n",
    "    * Create new columns\n",
    "    **df.cols().append()**\n",
    "    * Select columns\n",
    "    **df.cols().filter()**\n",
    "    * Between columns. We do not have any yet\n",
    "    * In column \n",
    "        * Transform\n",
    "        **df.cols().row(), trim(), reverse()...**\n",
    "        * Aggregation\n",
    "        **df.cols.min(), max()...**\n",
    "    * Delete column\n",
    "        **df.cols().drop()...**\n",
    "* Row\n",
    "    * Crete row\n",
    "    **df.row.append()**\n",
    "    * Select row\n",
    "    **df.row.filter()**\n",
    "    * In row operation\n",
    "    **where apply **\n",
    "    * Between rows\n",
    "    **df.groupBy**\n",
    "    * Delete row\n",
    "        **df.row().filter()**\n",
    "\n",
    "* Dataframe\n",
    "    * Show\n",
    "        **op.show()**\n",
    "    * Save\n",
    "        **op.save()**\n",
    "* Optimus\n",
    "    * Create\n",
    "        **op.create()**\n",
    "    * Load\n",
    "        **op.load()**\n",
    "    * Concat\n",
    "        **op.concat()**\n",
    "\n",
    "  \n",
    "### Why?\n",
    "\n",
    "\n",
    "* Pandas sum, sum all the column data but sub substract element between columns.\n",
    "* Pandas drop(axis = 1) to drop a column\n",
    "\n",
    "append a column vs append a row\n",
    "\n",
    "|Operation|Pandas|Spark|Optimus|\n",
    "|---|---|---|\n",
    "|Append Column|df.join(), pd.concat()|df.withColumn()|df.cols().append()|\n",
    "|Append Row|df.append()|df.union()|df.rows().append()|\n",
    "|Filter Column|df.filter( ,axis=1)|NA|df.cols().filter()|\n",
    "|Filter Row|df.filter()|df.filter()|df.rows().filter()|\n",
    "|Apply|df.apply()|udf, pandas_udf|df.cols().apply()|\n",
    "|Drop Column|df.drop()|NA|df.cols().drop()|\n",
    "|Drop Row|df.drop()|NA|df.rows().drop()|\n",
    "\n",
    "Replace \n",
    "\ta string or number insice a cell to a string or number\n",
    "\ta string or number(whole value) to a string or number\n",
    "\ta list of string or number to a specific number\n",
    "\ta datafreame column list of values to a specific string or number    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             ____        __  _                     \n",
      "            / __ \\____  / /_(_)___ ___  __  _______\n",
      "           / / / / __ \\/ __/ / __ `__ \\/ / / / ___/\n",
      "          / /_/ / /_/ / /_/ / / / / / / /_/ (__  ) \n",
      "          \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  \n",
      "              /_/                                  \n",
      "              \n",
      "Just checking that all necessary environments vars are present...\n",
      "-----\n",
      "PYSPARK_PYTHON=python\n",
      "SPARK_HOME=C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\n",
      "JAVA_HOME=C:\\java8\n",
      "-----\n",
      "Starting or getting SparkSession and SparkContext...\n",
      "Setting checkpoint folder ( local ). If you are in a cluster initialize optimus with master='your_ip' as param\n",
      "Deleting previous folder if exists...\n",
      "Creating the checkpoint directory...\n",
      "Optimus successfully imported. Have fun :).\n"
     ]
    }
   ],
   "source": [
    "# Create optimus\n",
    "op = Optimus(master=\"local\", app_name= \"optimus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-EP49A8K8:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>optimus</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2349f56b4a8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.get_ss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-EP49A8K8:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>optimus</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=optimus>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.get_sc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe\n",
    "### Spark\n",
    "\n",
    "This is ugly:\n",
    "\n",
    "```\n",
    "val someData = Seq(\n",
    "  Row(8, \"bat\"),\n",
    "  Row(64, \"mouse\"),\n",
    "  Row(-27, \"horse\")\n",
    ")\n",
    "\n",
    "val someSchema = List(\n",
    "  StructField(\"number\", IntegerType, true),\n",
    "  StructField(\"word\", StringType, true)\n",
    ")\n",
    "\n",
    "val someDF = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(someData),\n",
    "  StructType(someSchema)\n",
    ")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType, ArrayType\n",
    "\n",
    "df = op.create.df(\n",
    "    [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True),\n",
    "                (\"animals\", \"str\", True),\n",
    "                (\"thing\", StringType(), True),\n",
    "                (\"two strings\", StringType(), True),\n",
    "                (\"filter\", StringType(), True),\n",
    "                (\"num 2\", \"string\", True),\n",
    "                (\"date\", \"string\", True),\n",
    "                (\"num 3\", \"string\", True)\n",
    "                \n",
    "            ],[\n",
    "                (\"  I like     fish  \", 1, \"dog\", \"&^%$#housé\", \"cat-car\", \"a\",\"1\", \"20150510\", \"3\"),\n",
    "                (\"    zombies\", 2, \"cat\", \"tv\", \"dog-tv\", \"b\",\"2\", \"20160510\", \"3\"),\n",
    "                (\"simpsons   cat lady\", 2, \"frog\", \"table\",\"eagle-tv-plus\",\"1\",\"3\", \"20170510\", \"4\"),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"c\",\"4\", \"20180510\", \"5\"),\n",
    "    \n",
    "            ]\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat\n",
    "### Spark\n",
    "No available in Spark Vanilla\n",
    "\n",
    "### Pandas\n",
    "Almost the same functionlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|              words|num|animals|     thing|  two strings|filter|num 2|    date|num 3|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "|  I like     fish  |  1|    dog|&^%$#housé|      cat-car|     a|    1|20150510|    3|\n",
      "|            zombies|  2|    cat|        tv|       dog-tv|     b|    2|20160510|    3|\n",
      "|simpsons   cat lady|  2|   frog|     table|eagle-tv-plus|     1|    3|20170510|    4|\n",
      "|               null|  3|  eagle|     glass|      lion-pc|     c|    4|20180510|    5|\n",
      "+-------------------+---+-------+----------+-------------+------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op.concat(df,df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  1|  2|\n",
      "|  b|  3|  4|\n",
      "|  c|  5|  6|\n",
      "+---+---+---+\n",
      "\n",
      "+---+--------+-----+\n",
      "|  A|variable|value|\n",
      "+---+--------+-----+\n",
      "|  a|       B|    1|\n",
      "|  a|       C|    2|\n",
      "|  b|       B|    3|\n",
      "|  b|       C|    4|\n",
      "|  c|       B|    5|\n",
      "|  c|       C|    6|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                   'B': {0: 1, 1: 3, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})\n",
    "\n",
    "sdf = op.get_ss().createDataFrame(pdf)\n",
    "sdf.show()\n",
    "sdf.melt(id_vars=['A'], value_vars=['B', 'C']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = op.read.csv(\"Meteorite_Landings.csv\", header=True)\n",
    "df = df.cols().cast([('reclat', 'int')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-87.0,\n",
       "  -70.2,\n",
       "  -53.4,\n",
       "  -36.599999999999994,\n",
       "  -19.799999999999997,\n",
       "  -3.0,\n",
       "  13.800000000000011,\n",
       "  30.60000000000001,\n",
       "  47.400000000000006,\n",
       "  64.20000000000002,\n",
       "  81],\n",
       " [22098, 2, 40, 1171, 92, 6545, 5840, 2118, 475, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hist('reclat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
