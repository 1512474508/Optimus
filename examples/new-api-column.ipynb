{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimus\n",
    "op = Optimus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe\n",
    "### Spark\n",
    "\n",
    "This is ugly:\n",
    "\n",
    "```\n",
    "val someData = Seq(\n",
    "  Row(8, \"bat\"),\n",
    "  Row(64, \"mouse\"),\n",
    "  Row(-27, \"horse\")\n",
    ")\n",
    "\n",
    "val someSchema = List(\n",
    "  StructField(\"number\", IntegerType, true),\n",
    "  StructField(\"word\", StringType, true)\n",
    ")\n",
    "\n",
    "val someDF = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(someData),\n",
    "  StructType(someSchema)\n",
    ")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|        col_array|  col_int|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|    [baby, sorry]|[1, 2, 3]|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|[baby 1, sorry 1]|   [3, 4]|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|[baby 2, sorry 2]|[5, 6, 7]|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|[baby 3, sorry 3]|   [7, 8]|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType, ArrayType\n",
    "\n",
    "df = op.create.df(\n",
    "            [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True),\n",
    "                (\"animals\", \"str\", True),\n",
    "                (\"thing\", StringType(), True),\n",
    "                (\"two strings\", StringType(), True),\n",
    "                (\"filter\", StringType(), True),\n",
    "                (\"num 2\", \"string\", True),\n",
    "                (\"col_array\",  ArrayType(StringType()), True),\n",
    "                (\"col_int\",  ArrayType(IntegerType()), True)\n",
    "\n",
    "            ]\n",
    ",\n",
    "[\n",
    "                (\"  I like     fish  \", 1, \"dog\", \"housé\", \"cat-car\", \"a\",\"1\",[\"baby\", \"sorry\"],[1,2,3]),\n",
    "                (\"    zombies\", 2, \"cat\", \"tv\", \"dog-tv\", \"b\",\"2\",[\"baby 1\", \"sorry 1\"],[3,4]),\n",
    "                (\"simpsons   cat lady\", 2, \"frog\", \"table\",\"eagle-tv-plus\",\"1\",\"3\", [\"baby 2\", \"sorry 2\"], [5,6,7]),\n",
    "                (None, 3, \"eagle\", \"glass\", \"lion-pc\", \"c\",\"4\", [\"baby 3\", \"sorry 3\"] ,[7,8])\n",
    "            ])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Columns\n",
    "### Spark\n",
    "* You can not create multiple columns at the same time\n",
    "* You need to use the lit function. lit???\n",
    "\n",
    "### Pandas\n",
    "* Assing function seems to do the job https://stackoverflow.com/questions/12555323/adding-new-column-to-existing-dataframe-in-python-pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.cols().append(\"new_col_1\", 1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+---------+---------+---------+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|new_col_2|new_col_3|new_col_4|new_col_5|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+---------+---------+---------+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|     2.22|        3|     test|        2|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|     2.22|        3|     test|        4|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|     2.22|        3|     test|        4|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|     2.22|        3|     test|        6|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.cols().append([\n",
    "    (\"new_col_2\", 2.22),\n",
    "    (\"new_col_3\", lit(3)),\n",
    "    (\"new_col_4\", \"test\"),\n",
    "    (\"new_col_5\", df['num']*2)\n",
    "    ]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns\n",
    "### Spark\n",
    "* You can not select columns by string and index at the same time\n",
    "\n",
    "### Pandas\n",
    "* You can not select columns by string and index at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------------+\n",
      "|              words|num|animals|thing|              words|\n",
      "+-------------------+---+-------+-----+-------------------+\n",
      "|  I like     fish  |  1|    dog|housé|  I like     fish  |\n",
      "|            zombies|  2|    cat|   tv|            zombies|\n",
      "|simpsons   cat lady|  2|   frog|table|simpsons   cat lady|\n",
      "|               null|  3|  eagle|glass|               null|\n",
      "+-------------------+---+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"words\", 1, \"animals\", 3, 0]\n",
    "df.cols().filter(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "|num|num 2|new_col_1|\n",
      "+---+-----+---------+\n",
      "|  1|    1|        1|\n",
      "|  2|    2|        1|\n",
      "|  2|    3|        1|\n",
      "|  3|    4|        1|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().filter(\"n.*\", regex = True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------------+-------+-----+-----+\n",
      "|filter|  two strings|              words|animals|thing|num 2|\n",
      "+------+-------------+-------------------+-------+-----+-----+\n",
      "|     a|      cat-car|  I like     fish  |    dog|housé|    1|\n",
      "|     b|       dog-tv|            zombies|    cat|   tv|    2|\n",
      "|     1|eagle-tv-plus|simpsons   cat lady|   frog|table|    3|\n",
      "|     c|      lion-pc|               null|  eagle|glass|    4|\n",
      "+------+-------------+-------------------+-------+-----+-----+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().filter(\"*\", data_type = \"str\").show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Column\n",
    "### Spark\n",
    "You can not rename multiple columns using Spark Vanilla API\n",
    "\n",
    "\n",
    "### Pandas\n",
    "* Almost the same behavior https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+-----+-------------+------+-----+---------+\n",
      "|              words|number|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+------+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |     1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|     2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|     2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|     3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+------+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().rename([('num','number')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().rename(func = str.lower).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              WORDS|NUM|ANIMALS|THING|  TWO STRINGS|FILTER|NUM 2|NEW_COL_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().rename(func = str.upper).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast a columns\n",
    "\n",
    "### Spark\n",
    "* Can not cast multiple columns\n",
    "\n",
    "### Pandas\n",
    "This is a opinionated way to handle column casting. \n",
    "One of the first thing that every data cleaning process need to acomplish is define a data dictionary.\n",
    "Because of that we prefer to create a tuple like this:\n",
    "\n",
    "df.cols().cast(\n",
    "[(\"words\",\"str\"),\n",
    "(\"num\",\"int\"),\n",
    "(\"animals\",\"float\"),\n",
    "(\"thing\",\"str\")]\n",
    ")\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words: string, num: string, animals: string, thing: string, two strings: string, filter: string, num 2: int, new_col_1: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cols().cast([(\"num\", \"string\"),(\"num 2\", \"integer\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep columns\n",
    "### Spark\n",
    "* You can not remove multiple columns\n",
    "\n",
    "### Pandas\n",
    "* Handle in pandas with drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words: string, num: string, animals: string, thing: string, two strings: string, filter: string, num 2: string, new_col_1: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn(\"num\", col(\"num\").cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().keep(\"num\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move columns\n",
    "### Spark\n",
    "Do not exist in spark\n",
    "\n",
    "### Pandas\n",
    "Do not exist in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+-------------------+-------------+------+-----+---------+\n",
      "|num|animals|thing|              words|  two strings|filter|num 2|new_col_1|\n",
      "+---+-------+-----+-------------------+-------------+------+-----+---------+\n",
      "|  1|    dog|housé|  I like     fish  |      cat-car|     a|    1|        1|\n",
      "|  2|    cat|   tv|            zombies|       dog-tv|     b|    2|        1|\n",
      "|  2|   frog|table|simpsons   cat lady|eagle-tv-plus|     1|    3|        1|\n",
      "|  3|  eagle|glass|               null|      lion-pc|     c|    4|        1|\n",
      "+---+-------+-----+-------------------+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().move(\"words\", \"thing\", \"after\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Columns\n",
    "### Spark\n",
    "You can not sort columns using Spark Vanilla API \n",
    "\n",
    "### Pandas\n",
    "Similar to pandas\n",
    "http://pandas.pydata.org/pandas-docs/version/0.19/generated/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+---+-----+-----+-------------+-------------------+\n",
      "|animals|filter|new_col_1|num|num 2|thing|  two strings|              words|\n",
      "+-------+------+---------+---+-----+-----+-------------+-------------------+\n",
      "|    dog|     a|        1|  1|    1|housé|      cat-car|  I like     fish  |\n",
      "|    cat|     b|        1|  2|    2|   tv|       dog-tv|            zombies|\n",
      "|   frog|     1|        1|  2|    3|table|eagle-tv-plus|simpsons   cat lady|\n",
      "|  eagle|     c|        1|  3|    4|glass|      lion-pc|               null|\n",
      "+-------+------+---------+---+-----+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().sort().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-----+-----+---+---------+------+-------+\n",
      "|              words|  two strings|thing|num 2|num|new_col_1|filter|animals|\n",
      "+-------------------+-------------+-----+-----+---+---------+------+-------+\n",
      "|  I like     fish  |      cat-car|housé|    1|  1|        1|     a|    dog|\n",
      "|            zombies|       dog-tv|   tv|    2|  2|        1|     b|    cat|\n",
      "|simpsons   cat lady|eagle-tv-plus|table|    3|  2|        1|     1|   frog|\n",
      "|               null|      lion-pc|glass|    4|  3|        1|     c|  eagle|\n",
      "+-------------------+-------------+-----+-----+---+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().sort(order = \"desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns\n",
    "### Spark \n",
    "* You can not delete multiple colums\n",
    "\n",
    "### Pandas\n",
    "* Almost the same as pandas\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------+------+-----+---------+\n",
      "|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------+-----+-------------+------+-----+---------+\n",
      "|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.cols().drop(\"num\")\n",
    "df2 = df.cols().drop([\"num\",\"words\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "cols y rows functions are used to organize and encapsulate optimus' functionality apart of Apache Spark Dataframe API. This have a disadvantage at chaining time because we need to user invoke cols or rows in every step.\n",
    "\n",
    "At the same time it can be helpfull when you look at the code because every line is self explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+---------+---------+------+-------+\n",
      "|  two strings|thing|num 2|new_col_2|new_col_1|filter|animals|\n",
      "+-------------+-----+-----+---------+---------+------+-------+\n",
      "|      cat-car|housé|    1|spongebob|        1|     a|    dog|\n",
      "|       dog-tv|   tv|    2|spongebob|        1|     b|    cat|\n",
      "|eagle-tv-plus|table|    3|spongebob|        1|     1|   frog|\n",
      "|      lion-pc|glass|    4|spongebob|        1|     c|  eagle|\n",
      "+-------------+-----+-----+---------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .cols().rename([('num','number')])\\\n",
    "    .cols().drop([\"number\",\"words\"])\\\n",
    "    .withColumn(\"new_col_2\", lit(\"spongebob\"))\\\n",
    "    .cols().append(\"new_col_1\", 1)\\\n",
    "    .cols().sort(order= \"desc\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Columns\n",
    "### Spark\n",
    "\n",
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+-----+-----+-----+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|COL_0|COL_1|COL_2|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----+-----+-----+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|  cat|  car| null|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|  dog|   tv| null|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|eagle|   tv| plus|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4| lion|   pc| null|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().split(\"two strings\",\"-\", n=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+-----+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|COL_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+-----+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|  car|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|   tv|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|   tv|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|   pc|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().split(\"two strings\",\"-\", get = 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute\n",
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |1.0|    dog|housé|      cat-car|     a|  1.0|        1|\n",
      "|            zombies|2.0|    cat|   tv|       dog-tv|     b|  2.0|        1|\n",
      "|simpsons   cat lady|2.0|   frog|table|eagle-tv-plus|     1|  3.0|        1|\n",
      "|               null|3.0|  eagle|glass|      lion-pc|     c|  4.0|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast =df.cols().cast([(\"num\",\"double\"),(\"num 2\", \"double\")])\n",
    "df_cast.dtypes\n",
    "\n",
    "df_cast.cols().impute([\"num\",\"num 2\"], [\"out_a\",\"out_B\"], strategy=\"mean\")\n",
    "df_cast.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get columns by type\n",
    "### Spark\n",
    "\n",
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|num|new_col_1|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        1|\n",
      "|  2|        1|\n",
      "|  3|        1|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().filter_by_dtypes(\"int\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply custom function\n",
    "### Spark\n",
    "You need to declare a UDF Spark function\n",
    "\n",
    "### Pandas\n",
    "Almost the same behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o322.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 24, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-dfa48055f9ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_by_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"string\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"10\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_by_data_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"integer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o322.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 24, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "def sum_(val, attr):\n",
    "    return attr\n",
    "\n",
    "df.cols().apply_by_dtypes(\"filter\", sum_, \"string\", \"10\", data_type=\"integer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'UDFs' to process column 'num'\n",
      "Using 'UDFs' to process column 'num'\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  | 65|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies| 66|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady| 66|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null| 67|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(val, attr):\n",
    "    return val + attr\n",
    "\n",
    "df.cols().apply([\"num\", \"num\"], func, \"int\", 32 ,\"udf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o414.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 26, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-3a64fd17ed18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moptimus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfilter_row_by_data_type\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfbdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfbdt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"integer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o414.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 26, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 219, in main\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 139, in read_udfs\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 119, in read_single_udf\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 59, in read_command\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 559, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'optimus'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from optimus.functions import filter_row_by_data_type as fbdt\n",
    "\n",
    "df.where(fbdt(\"filter\", \"integer\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from optimus.functions import abstract_udf as audf \n",
    "\n",
    "def func(val, attr):\n",
    "    return val>1\n",
    "\n",
    "df.where(audf(\"num\", func, \"bool\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  | 21|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies| 22|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady| 22|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null| 23|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(val, attr):\n",
    "    return val + attr\n",
    "\n",
    "df.cols().apply([\"num\", \"num\"], func, \"int\", 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  | 11|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies| 12|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady| 12|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null| 13|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/31400143/column-filtering-in-pyspark\n",
    "from optimus.functions import abstract_udf as audf \n",
    "\n",
    "def func(val, attr):\n",
    "    return val+attr[0]\n",
    "\n",
    "df.withColumn(\"num\", audf (\"num\", func, \"int\", [10,20])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  2|    dog|housé|      cat-car|     a|    2|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    2|        1|\n",
      "|               null|  2|  eagle|glass|      lion-pc|     c|    2|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def func(col_name, attr):\n",
    "    # return F.col(col_name) + 1\n",
    "    return F.when(F.col(col_name)>0 ,2)\n",
    "\n",
    "df.cols().apply_exp([\"num\",\"num 2\"], func).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    DOG|housé|      CAT-CAR|     a|    1|        1|\n",
      "|            zombies|  2|    CAT|   tv|       DOG-TV|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   FROG|table|EAGLE-TV-PLUS|     1|    3|        1|\n",
      "|               null|  3|  EAGLE|glass|      LION-PC|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def func(col_name, attr):\n",
    "    return F.upper(F.col(col_name))\n",
    "\n",
    "df.cols().apply_exp([\"two strings\",\"animals\"], func).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies| 10|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady| 10|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null| 10|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  | 10|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies| 10|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady| 10|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null| 10|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(val, attr):\n",
    "    return 10\n",
    "\n",
    "col = \"num\"\n",
    "\n",
    "df.cols().apply(col, func, \"int\", when= df[\"num\"]>1).show()\n",
    "\n",
    "df.cols().apply(col, func, \"int\", when= fbdt(col, \"integer\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Nulls\n",
    "### Spark\n",
    "\n",
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_null = op.get_ss().createDataFrame(\n",
    "    [(1, 1, None), (1, 2, float(5)), (1, 3, np.nan), (1, 4, None), (1, 5, float(10)), (1, 6, float('nan')), (1, 6, float('nan'))],\n",
    "    ('session', \"timestamp1\", \"id2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id2': 5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null.cols().count_na(\"id2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id2': 5, 'session': 0, 'timestamp1': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null.cols().count_na(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count uniques\n",
    "### Spark\n",
    "\n",
    "### Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'animals': 4,\n",
       " 'filter': 4,\n",
       " 'new_col_1': 1,\n",
       " 'num': 3,\n",
       " 'num 2': 4,\n",
       " 'thing': 4,\n",
       " 'two strings': 4,\n",
       " 'words': 4}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cols().count_uniques(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique\n",
    "### Spark\n",
    "An abstraction of distinct to be use in multiple columns at the same time\n",
    "\n",
    "### Pandas\n",
    "Similar behavior than pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_distinct = op.create.df(\n",
    "            [\n",
    "                (\"words\", \"str\", True),\n",
    "                (\"num\", \"int\", True)\n",
    "            ],\n",
    "[\n",
    "                (\"  I like     fish  \", 1),\n",
    "                (\"    zombies\", 2),\n",
    "                (\"simpsons   cat lady\", 2),\n",
    "                (None, 3),\n",
    "                  (None, 0)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  2|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_distinct\\\n",
    "    .select(\"num\")\\\n",
    "    .cols().unique().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+\n",
      "|              words|num|\n",
      "+-------------------+---+\n",
      "|  I like     fish  |  1|\n",
      "|            zombies|  2|\n",
      "|simpsons   cat lady|  2|\n",
      "|               null|  3|\n",
      "|               null|  0|\n",
      "+-------------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num': 1, 'words': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zeros = df_distinct\n",
    "df_zeros.show()\n",
    "df_zeros.cols().count_zeros(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'animals': 'string',\n",
       " 'filter': 'string',\n",
       " 'new_col_1': 'int',\n",
       " 'num': 'int',\n",
       " 'num 2': 'string',\n",
       " 'thing': 'string',\n",
       " 'two strings': 'string',\n",
       " 'words': 'string'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cols().dtypes('*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|animals|   tv|      animals|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|animals|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|animals|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|animals|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|animals|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|animals|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1| animal|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies| 10|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady| 10|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null| 10|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|        1|\n",
      "|            zombies|  6|    cat|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   cat lady|  6|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  6|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().replace([\"two strings\",\"animals\"], [\"dog-tv\", \"cat\", \"eagle\", \"fish\"], \"animals\").show()\n",
    "\n",
    "df.cols().replace('animals',[\"dog\",\"cat\"],\"animals\").show()\n",
    "\n",
    "df.cols().replace('animals',[(\"dog\",\"animals\"),(\"cat\",\"animals\")]).show()\n",
    "\n",
    "df.cols().replace('animals',\"dog\",'animal').show()\n",
    "\n",
    "df.cols().replace('num',[\"3\",2], 10).show()\n",
    "\n",
    "df.cols().replace('num',[(\"3\",6),(2,6)]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|  I like     fish  |  1|    dog|housé|       animal|     a|    1|        1|\n",
      "|            zombies|  2| animal|   tv|       dog-tv|     b|    2|        1|\n",
      "|             animal|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n",
      "+--------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|               words|num|animals|thing|  two strings|filter|num 2|new_col_1|\n",
      "+--------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "|   I like     fish  |  1|    dog|housé|   animal-car|     a|    1|        1|\n",
      "|             zombies|  2| animal|   tv|       dog-tv|     b|    2|        1|\n",
      "|simpsons   animal...|  2|   frog|table|eagle-tv-plus|     1|    3|        1|\n",
      "|                null|  3|  eagle|glass|      lion-pc|     c|    4|        1|\n",
      "+--------------------+---+-------+-----+-------------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().replace('*','.*[Cc]at.*', 'animal', regex=True).show()\n",
    "df.cols().replace('*','cat', 'animal', regex=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"array\" in \"array<string>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('words', 'string'),\n",
       " ('num', 'int'),\n",
       " ('animals', 'string'),\n",
       " ('thing', 'string'),\n",
       " ('two strings', 'string'),\n",
       " ('filter', 'string'),\n",
       " ('num 2', 'string'),\n",
       " ('col_array', 'array<string>')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+------+-------+-----+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|        col_array| COL_0|  COL_1|COL_2|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+------+-------+-----+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|    [baby, sorry]|  baby|  sorry| null|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|[baby 1, sorry 1]|baby 1|sorry 1| null|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|[baby 2, sorry 2]|baby 2|sorry 2| null|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|[baby 3, sorry 3]|baby 3|sorry 3| null|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().unnest(\"col_array\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('words', 'string'),\n",
       " ('num', 'int'),\n",
       " ('animals', 'string'),\n",
       " ('thing', 'string'),\n",
       " ('two strings', 'string'),\n",
       " ('filter', 'string'),\n",
       " ('num 2', 'string'),\n",
       " ('col_array', 'array<string>')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'words': StringType},\n",
       " {'num': IntegerType},\n",
       " {'animals': StringType},\n",
       " {'thing': StringType},\n",
       " {'two strings': StringType},\n",
       " {'filter': StringType},\n",
       " {'num 2': StringType},\n",
       " {'col_array': ArrayType(StringType,true)}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cols().dtypes(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+---------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|        col_array|  col_int|   nested|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+---------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|    [baby, sorry]|[1, 2, 3]|[1.0,1.0]|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|[baby 1, sorry 1]|   [3, 4]|[2.0,2.0]|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|[baby 2, sorry 2]|[5, 6, 7]|[2.0,2.0]|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|[baby 3, sorry 3]|   [7, 8]|[3.0,3.0]|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().nest([\"num\", \"num\"], \"nested\",shape=\"vector\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animals', 'two strings']\n",
      "Using 'Column Expression' to process column 'nested'\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+------------------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|        col_array|  col_int|            nested|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+------------------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|    [baby, sorry]|[1, 2, 3]|       dog cat-car|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|[baby 1, sorry 1]|   [3, 4]|        cat dog-tv|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|[baby 2, sorry 2]|[5, 6, 7]|frog eagle-tv-plus|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|[baby 3, sorry 3]|   [7, 8]|     eagle lion-pc|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().nest([\"animals\", \"two strings\"], [\"nested\"],shape=\"string\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'Column Expression' to process column 'nested'\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+--------------------+\n",
      "|              words|num|animals|thing|  two strings|filter|num 2|        col_array|  col_int|              nested|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+--------------------+\n",
      "|  I like     fish  |  1|    dog|housé|      cat-car|     a|    1|    [baby, sorry]|[1, 2, 3]|      [dog, cat-car]|\n",
      "|            zombies|  2|    cat|   tv|       dog-tv|     b|    2|[baby 1, sorry 1]|   [3, 4]|       [cat, dog-tv]|\n",
      "|simpsons   cat lady|  2|   frog|table|eagle-tv-plus|     1|    3|[baby 2, sorry 2]|[5, 6, 7]|[frog, eagle-tv-p...|\n",
      "|               null|  3|  eagle|glass|      lion-pc|     c|    4|[baby 3, sorry 3]|   [7, 8]|    [eagle, lion-pc]|\n",
      "+-------------------+---+-------+-----+-------------+------+-----+-----------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cols().nest([\"animals\", \"two strings\"], [\"nested\"],shape=\"array\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnest\n",
    "Works with arrays, Vectors and strings(ala split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|  two strings|two strings_0|two strings_1|two strings_2|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|      cat-car|          cat|          car|         null|\n",
      "|       dog-tv|          dog|           tv|         null|\n",
      "|eagle-tv-plus|        eagle|           tv|         plus|\n",
      "|      lion-pc|         lion|           pc|         null|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .cols().unnest([\"col_array\"])\\\n",
    "    .cols().unnest([\"col_int\"])\\\n",
    "    .cols().unnest([\"two strings\"], n= 3, mark = \"-\")\\\n",
    "    .cols().filter(\"two.*\", regex = True)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---+---+---+\n",
      "|   word|       vector| _3| _4| _5|\n",
      "+-------+-------------+---+---+---+\n",
      "| assert|[1.0,2.0,3.0]|1.0|2.0|3.0|\n",
      "|require|(3,[1],[2.0])|0.0|2.0|0.0|\n",
      "+-------+-------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df1 = op.sc.parallelize([\n",
    "    (\"assert\", Vectors.dense([1, 2, 3])),\n",
    "    (\"require\", Vectors.sparse(3, {1: 2}))\n",
    "]).toDF([\"word\", \"vector\"])\n",
    "\n",
    "\n",
    "df1\\\n",
    "    .cols().unnest([\"vector\"])\\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
