{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = Optimus(master='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date, datetime\n",
    "\n",
    "df = op.create.df(\n",
    "    [\n",
    "        (\"names\", \"str\"),\n",
    "        (\"height(ft)\", ShortType()),\n",
    "        (\"function\", \"str\"),\n",
    "        (\"rank\", ByteType()),\n",
    "        (\"age\", \"int\"),\n",
    "        (\"weight(t)\", \"float\"),\n",
    "        \"japanese name\",\n",
    "        \"last position seen\",\n",
    "        \"date arrival\",\n",
    "        \"last date seen\",\n",
    "        (\"attributes\", ArrayType(FloatType())),\n",
    "        (\"DateType\", DateType()),\n",
    "        (\"Tiemstamp\", TimestampType()),\n",
    "        (\"Cybertronian\", BooleanType()),\n",
    "        (\"function(binary)\", BinaryType()),\n",
    "        (\"NullType\", NullType())\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        (\"Optim'us\", 28, \"Leader\", 10, 5000000, 4.30, [\"Inochi\", \"Convoy\"], \"19.442735,-99.201111\", \"1980/04/10\",\n",
    "         \"2016/09/10\", [8.5344, 4300.0], date(2016, 9, 10), datetime(2014, 6, 24), True, bytearray(\"Leader\", \"utf-8\"),\n",
    "         None),\n",
    "        (\"bumbl#ebéé  \", 17, \"Espionage\", 7, 5000000, 2.0, [\"Bumble\", \"Goldback\"], \"10.642707,-71.612534\", \"1980/04/10\",\n",
    "         \"2015/08/10\", [5.334, 2000.0], date(2015, 8, 10), datetime(2014, 6, 24), True, bytearray(\"Espionage\", \"utf-8\"),\n",
    "         None),\n",
    "        (\"ironhide&\", 26, \"Security\", 7, 5000000, 4.0, [\"Roadbuster\"], \"37.789563,-122.400356\", \"1980/04/10\",\n",
    "         \"2014/07/10\", [7.9248, 4000.0], date(2014, 6, 24), datetime(2014, 6, 24), True, bytearray(\"Security\", \"utf-8\"),\n",
    "         None),\n",
    "        (\"Jazz\", 13, \"First Lieutenant\", 8, 5000000, 1.80, [\"Meister\"], \"33.670666,-117.841553\", \"1980/04/10\",\n",
    "         \"2013/06/10\", [3.9624, 1800.0], date(2013, 6, 24), datetime(2014, 6, 24), True,\n",
    "         bytearray(\"First Lieutenant\", \"utf-8\"), None),\n",
    "        (\"Megatron\", None, \"None\", 10, 5000000, 5.70, [\"Megatron\"], None, \"1980/04/10\", \"2012/05/10\", [None, 5700.0],\n",
    "         date(2012, 5, 10), datetime(2014, 6, 24), True, bytearray(\"None\", \"utf-8\"), None),\n",
    "        (\"Metroplex_)^$\", 300, \"Battle Station\", 8, 5000000, None, [\"Metroflex\"], None, \"1980/04/10\", \"2011/04/10\",\n",
    "         [91.44, None], date(2011, 4, 10), datetime(2014, 6, 24), True, bytearray(\"Battle Station\", \"utf-8\"), None),\n",
    "\n",
    "    ])\n",
    "df.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus.helpers.test import Test\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Test(df, \"DataFrameCols\", imports=[\"from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\",\n",
    "                                       \"import numpy.nan as nan\",\n",
    "                                       \"import datetime\",\n",
    "                                       \"from pyspark.sql import functions as F\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def func(col_name, attrs):\n",
    "    return F.col(col_name) * 2\n",
    "\n",
    "\n",
    "numeric_col = \"height(ft)\"\n",
    "numeric_col_B = \"rank\"\n",
    "numeric_col_C = \"rank\"\n",
    "string_col = \"function\"\n",
    "data_col = \"date arrival\"\n",
    "data_col_B = \"last date seen\"\n",
    "new_col = \"new col\"\n",
    "array_col = \"attributes\"\n",
    "\n",
    "t.run(\n",
    "\n",
    "    t.create(None, \"cols.min\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.min\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.max\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.max\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.range\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.range\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.median\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.median\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.percentile\", None, \"json\", numeric_col, [0.05, 0.25], 1),\n",
    "    t.create(None, \"cols.percentile\", \"all_columns\", \"json\", \"*\", [0.05, 0.25], 1),\n",
    "\n",
    "    t.create(None, \"cols.mad\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.mad\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.std\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.std\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.kurt\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.kurt\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.mean\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.mean\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.skewness\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.skewness\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.sum\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.sum\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.variance\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.variance\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.abs\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.abs\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.mode\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.mode\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.count\", None, \"json\"),\n",
    "\n",
    "    t.create(None, \"cols.count_na\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.count_na\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.count_zeros\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.count_zeros\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.count_uniques\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.count_uniques\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.unique\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.unique\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.add\", None, \"df\", [numeric_col, numeric_col_B]),\n",
    "    t.create(None, \"cols.add\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.sub\", None, \"df\", [numeric_col, numeric_col_B]),\n",
    "    t.create(None, \"cols.sub\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.mul\", None, \"df\", [numeric_col, numeric_col_B]),\n",
    "    t.create(None, \"cols.mul\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.div\", None, \"df\", [numeric_col, numeric_col_B]),\n",
    "    t.create(None, \"cols.div\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.z_score\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.z_score\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.iqr\", None, \"json\", numeric_col),\n",
    "    t.create(None, \"cols.iqr\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.lower\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.lower\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.upper\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.upper\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.trim\", None, \"df\", numeric_col),\n",
    "\n",
    "    t.create(None, \"cols.trim\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.reverse\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.reverse\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.remove_accents\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.remove_accents\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.remove_special_chars\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.remove_special_chars\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.remove_white_spaces\", None, \"df\", numeric_col),\n",
    "    t.create(None, \"cols.remove_white_spaces\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.date_transform\", None, \"df\", data_col, \"yyyy/MM/dd\", \"dd-MM-YYYY\"),\n",
    "    t.create(None, \"cols.date_transform\", \"all_columns\", \"df\", [data_col, data_col_B], \"yyyy/MM/dd\", \"dd-MM-YYYY\"),\n",
    "\n",
    "    t.create(None, \"cols.years_between\", None, \"df\", data_col, \"yyyyMMdd\"),\n",
    "    t.create(None, \"cols.years_between\", \"multiple_columns\", \"df\", [data_col, data_col_B], \"yyyyMMdd\"),\n",
    "\n",
    "    # ---\n",
    "\n",
    "    t.create(None, \"cols.impute\", None, \"df\", numeric_col_B),\n",
    "    t.create(None, \"cols.impute\", \"all_columns\", \"df\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.hist\", None, \"json\", numeric_col_B, 4),\n",
    "    #t.create(None,\"cols.hist\",\"all_columns\",\"df\",\"*\",4),\n",
    "\n",
    "    t.create(None, \"cols.frequency\", None, \"json\", numeric_col_B, 4),\n",
    "    t.create(None, \"cols.frequency\", \"all_columns\", \"json\", \"*\", 4),\n",
    "\n",
    "    t.create(None, \"cols.schema_dtype\", None, \"json\", numeric_col_B),\n",
    "    #t.create(None, \"cols.schema_dtype\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.dtypes\", None, \"json\", numeric_col_B),\n",
    "    t.create(None, \"cols.dtypes\", \"all_columns\", \"json\", \"*\"),\n",
    "\n",
    "    t.create(None, \"cols.select_by_dtypes\", \"str\", \"df\", \"str\"),\n",
    "    t.create(None, \"cols.select_by_dtypes\", \"int\", \"df\", \"int\"),\n",
    "    t.create(None, \"cols.select_by_dtypes\", \"float\", \"df\", \"float\"),\n",
    "    t.create(None, \"cols.select_by_dtypes\", \"array\", \"df\", \"array\"),\n",
    "\n",
    "    t.create(None, \"cols.names\", None, \"json\"),\n",
    "\n",
    "    t.create(None, \"cols.qcut\", None, \"df\", numeric_col_B, 4),\n",
    "    t.create(None, \"cols.qcut\", \"all_columns\", \"df\", \"*\", 4),\n",
    "\n",
    "    t.create(None, \"cols.clip\", None, \"df\", numeric_col_B, 3, 5),\n",
    "    t.create(None, \"cols.clip\", \"all_columns\", \"df\", \"*\", 3, 5),\n",
    "\n",
    "    t.create(None, \"cols.replace\", None, \"df\", string_col, [(\"Security\", \"Leader\")], \"Match\"),\n",
    "    t.create(None, \"cols.replace\", \"all_columns\", \"df\", \"*\", [(\"Jazz\", \"Leader\")], \"Match\"),\n",
    "\n",
    "    t.create(None, \"cols.apply_expr\", None, \"df\", numeric_col_B, func),\n",
    "    t.create(None, \"cols.apply_expr\", \"all_columns\", \"df\", [numeric_col_B,numeric_col_C], func),\n",
    "\n",
    "    t.create(None, \"cols.append\", \"number\", \"df\", new_col, 1),\n",
    "\n",
    "    t.create(None, \"cols.append\", \"advance\", \"df\", [(\"new_col_4\", \"test\"),\n",
    "                                                    (\"new_col_5\", df[numeric_col_B] * 2),\n",
    "                                                    (\"new_col_6\", [1, 2, 3])\n",
    "                                                    ]),\n",
    "\n",
    "    t.create(None, \"cols.rename\", None, \"df\", numeric_col_B, numeric_col_B + \"(old)\"),\n",
    "    t.create(None, \"cols.rename\", \"list\", \"df\",\n",
    "             [numeric_col, numeric_col + \"(tons)\", numeric_col_B, numeric_col_B + \"(old)\"]),\n",
    "    t.create(None, \"cols.rename\", \"function\", \"df\", str.upper),\n",
    "\n",
    "    t.create(None, \"cols.drop\", None, \"df\", numeric_col_B),\n",
    "\n",
    "    t.create(None, \"cols.cast\", None, \"df\", string_col, \"string\"),\n",
    "    t.create(None, \"cols.cast\", \"all_columns\", \"df\", \"*\", \"string\"),\n",
    "    t.create(None, \"cols.cast\", \"vector\", \"df\", array_col, Vectors),\n",
    "\n",
    "    t.create(None, \"cols.keep\", None, \"df\", numeric_col_B),\n",
    "\n",
    "    t.create(None, \"cols.move\", None, \"df\", numeric_col_B, \"after\", array_col),\n",
    "\n",
    "    t.create(None, \"cols.select\", None, \"df\", 0, numeric_col),\n",
    "\n",
    "    t.create(None, \"cols.select\", \"regex\", \"df\", \"n.*\", regex=True),\n",
    "\n",
    "    t.create(None, \"cols.sort\", None, \"df\"),\n",
    "    t.create(None, \"cols.sort\", \"desc\", \"df\", \"desc\"),\n",
    "    t.create(None, \"cols.sort\", \"asc\", \"df\", \"asc\"),\n",
    "\n",
    "    t.create(None, \"cols.fill_na\", None, \"df\", numeric_col, \"N/A\"),\n",
    "    t.create(None, \"cols.fill_na\", \"all_columns\", \"df\", \"*\", \"N/A\"),\n",
    "\n",
    "    t.create(None, \"cols.nest\", None, \"df\", [numeric_col, numeric_col_B], new_col, separator=\" \"),\n",
    "    t.create(None, \"cols.nest\", \"mix\", \"df\", [F.col(numeric_col), F.col(numeric_col_B)], \"E\", separator=\"--\"),\n",
    "\n",
    "    #t.create(None, \"cols.nest\", \"vector_all_columns\", \"df\", [numeric_col, numeric_col_B], new_col, shape=\"vector\"),\n",
    "    t.create(None, \"cols.nest\", \"vector\", \"df\", [numeric_col_C, numeric_col_B], new_col, shape=\"vector\"),\n",
    "\n",
    "    #t.create(None, \"cols.nest\", \"array_all_columns\", \"df\", \"*\", new_col, shape=\"array\"),\n",
    "    t.create(None, \"cols.nest\", \"array\", \"df\", [numeric_col, numeric_col_B,numeric_col_C], new_col, shape=\"array\"),\n",
    "\n",
    "    t.create(None, \"cols.unnest\", \"array_all_columns\", \"df\", array_col, \"-\", index=1),\n",
    "    t.create(None, \"cols.unnest\", \"array\", \"df\", array_col),\n",
    "    t.create(None, \"cols.unnest\", \"array_all_columns\", \"df\", array_col),\n",
    "\n",
    "    t.create(None, \"cols.is_na\", \"all_columns\", \"df\", \"*\"),\n",
    "    t.create(None, \"cols.is_na\", None, \"df\", numeric_col),\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext_formats": "ipynb,py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
